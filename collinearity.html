<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Statistics with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Applied Statistics with <code>R</code>">
  <meta name="generator" content="bookdown 0.2.3 and GitBook 2.6.7">

  <meta property="og:title" content="Applied Statistics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://daviddalpiaz.github.io/appliedstats/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/appliedstats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Statistics with R" />
  
  
  

<meta name="author" content="David Dalpiaz">


<meta name="date" content="2016-11-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="transformations.html">
<link rel="next" href="model-selection.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i><b>1.1</b> About This Book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>1.2</b> Conventions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.3</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.4</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to <code>R</code></a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-resources"><i class="fa fa-check"></i><b>2.1</b> <code>R</code> Resources</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-basics"><i class="fa fa-check"></i><b>2.2</b> <code>R</code> Basics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-calculations"><i class="fa fa-check"></i><b>2.2.1</b> Basic Calculations</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help"><i class="fa fa-check"></i><b>2.2.2</b> Getting Help</a></li>
<li class="chapter" data-level="2.2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-packages"><i class="fa fa-check"></i><b>2.2.3</b> Installing Packages</a></li>
<li class="chapter" data-level="2.2.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-types"><i class="fa fa-check"></i><b>2.2.4</b> Data Types</a></li>
<li class="chapter" data-level="2.2.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#vectors"><i class="fa fa-check"></i><b>2.2.5</b> Vectors</a></li>
<li class="chapter" data-level="2.2.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#summary-statistics"><i class="fa fa-check"></i><b>2.2.6</b> Summary Statistics</a></li>
<li class="chapter" data-level="2.2.7" data-path="introduction-to-r.html"><a href="introduction-to-r.html#matrices"><i class="fa fa-check"></i><b>2.2.7</b> Matrices</a></li>
<li class="chapter" data-level="2.2.8" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-frames"><i class="fa fa-check"></i><b>2.2.8</b> Data Frames</a></li>
<li class="chapter" data-level="2.2.9" data-path="introduction-to-r.html"><a href="introduction-to-r.html#plotting"><i class="fa fa-check"></i><b>2.2.9</b> Plotting</a></li>
<li class="chapter" data-level="2.2.10" data-path="introduction-to-r.html"><a href="introduction-to-r.html#distributions"><i class="fa fa-check"></i><b>2.2.10</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#programming-basics"><i class="fa fa-check"></i><b>2.3</b> Programming Basics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#logical-operators"><i class="fa fa-check"></i><b>2.3.1</b> Logical Operators</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#control-flow"><i class="fa fa-check"></i><b>2.3.2</b> Control Flow</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#functions"><i class="fa fa-check"></i><b>2.3.3</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#hypothesis-tests-in-r"><i class="fa fa-check"></i><b>2.4</b> Hypothesis Tests in <code>R</code></a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#one-sample-t-test-review"><i class="fa fa-check"></i><b>2.4.1</b> One Sample t-Test: Review</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#one-sample-t-test-example"><i class="fa fa-check"></i><b>2.4.2</b> One Sample t-Test: Example</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#two-sample-t-test-review"><i class="fa fa-check"></i><b>2.4.3</b> Two Sample t-Test: Review</a></li>
<li class="chapter" data-level="2.4.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#two-sample-t-test-example"><i class="fa fa-check"></i><b>2.4.4</b> Two Sample t-Test: Example</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a><ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#paired-differences"><i class="fa fa-check"></i><b>2.5.1</b> Paired Differences</a></li>
<li class="chapter" data-level="2.5.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#distribution-of-a-sample-mean"><i class="fa fa-check"></i><b>2.5.2</b> Distribution of a Sample Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#modeling"><i class="fa fa-check"></i><b>3.1</b> Modeling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>3.1.1</b> Simple Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#least-squares-approach"><i class="fa fa-check"></i><b>3.2</b> Least Squares Approach</a><ul>
<li class="chapter" data-level="3.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#making-predictions"><i class="fa fa-check"></i><b>3.2.1</b> Making Predictions</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals"><i class="fa fa-check"></i><b>3.2.2</b> Residuals</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#variance-estimation"><i class="fa fa-check"></i><b>3.2.3</b> Variance Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#decomposition-of-variation"><i class="fa fa-check"></i><b>3.3</b> Decomposition of Variation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>3.3.1</b> Coefficient of Determination</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-lm-function"><i class="fa fa-check"></i><b>3.4</b> The <code>lm</code> Function</a></li>
<li class="chapter" data-level="3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#maximum-likelihood-estimation-mle-approach"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation (MLE) Approach</a></li>
<li class="chapter" data-level="3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simulating-slr"><i class="fa fa-check"></i><b>3.6</b> Simulating SLR</a></li>
<li class="chapter" data-level="3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#history"><i class="fa fa-check"></i><b>3.7</b> History</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Inference for Simple Linear Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#gaussmarkov-theorem"><i class="fa fa-check"></i><b>4.1</b> Gauss–Markov Theorem</a></li>
<li class="chapter" data-level="4.2" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#sampling-distributions"><i class="fa fa-check"></i><b>4.2</b> Sampling Distributions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#simulating-sampling-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Simulating Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#standard-errors"><i class="fa fa-check"></i><b>4.3</b> Standard Errors</a></li>
<li class="chapter" data-level="4.4" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-intervals-for-slope-and-intercept"><i class="fa fa-check"></i><b>4.4</b> Confidence Intervals for Slope and Intercept</a></li>
<li class="chapter" data-level="4.5" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#hypothesis-tests"><i class="fa fa-check"></i><b>4.5</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="4.6" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#cars-example"><i class="fa fa-check"></i><b>4.6</b> <code>cars</code> Example</a><ul>
<li class="chapter" data-level="4.6.1" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#tests-in-r"><i class="fa fa-check"></i><b>4.6.1</b> Tests in <code>R</code></a></li>
<li class="chapter" data-level="4.6.2" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#significance-of-regression-t-test"><i class="fa fa-check"></i><b>4.6.2</b> Significance of Regression, t-Test</a></li>
<li class="chapter" data-level="4.6.3" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-intervals-in-r"><i class="fa fa-check"></i><b>4.6.3</b> Confidence Intervals in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-interval-for-mean-response"><i class="fa fa-check"></i><b>4.7</b> Confidence Interval for Mean Response</a></li>
<li class="chapter" data-level="4.8" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#prediction-interval-for-new-observations"><i class="fa fa-check"></i><b>4.8</b> Prediction Interval for New Observations</a></li>
<li class="chapter" data-level="4.9" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#confidence-and-prediction-bands"><i class="fa fa-check"></i><b>4.9</b> Confidence and Prediction Bands</a></li>
<li class="chapter" data-level="4.10" data-path="inference-for-simple-linear-regression.html"><a href="inference-for-simple-linear-regression.html#significance-of-regression-f-test"><i class="fa fa-check"></i><b>4.10</b> Significance of Regression, F-Test</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#matrix-approach-to-regression"><i class="fa fa-check"></i><b>5.1</b> Matrix Approach to Regression</a></li>
<li class="chapter" data-level="5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#sampling-distribution"><i class="fa fa-check"></i><b>5.2</b> Sampling Distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#single-parameter-tests"><i class="fa fa-check"></i><b>5.2.1</b> Single Parameter Tests</a></li>
<li class="chapter" data-level="5.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#confidence-intervals-for-mean-response"><i class="fa fa-check"></i><b>5.2.3</b> Confidence Intervals for Mean Response</a></li>
<li class="chapter" data-level="5.2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#prediction-intervals"><i class="fa fa-check"></i><b>5.2.4</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#signifiance-of-regression"><i class="fa fa-check"></i><b>5.3</b> Signifiance of Regression</a></li>
<li class="chapter" data-level="5.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#nested-models"><i class="fa fa-check"></i><b>5.4</b> Nested Models</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simulation-1"><i class="fa fa-check"></i><b>5.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building.html"><a href="model-building.html"><i class="fa fa-check"></i><b>6</b> Model Building</a><ul>
<li class="chapter" data-level="6.1" data-path="model-building.html"><a href="model-building.html#family-form-and-fit"><i class="fa fa-check"></i><b>6.1</b> Family, Form, and Fit</a><ul>
<li class="chapter" data-level="6.1.1" data-path="model-building.html"><a href="model-building.html#fit"><i class="fa fa-check"></i><b>6.1.1</b> Fit</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building.html"><a href="model-building.html#form"><i class="fa fa-check"></i><b>6.1.2</b> Form</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building.html"><a href="model-building.html#family"><i class="fa fa-check"></i><b>6.1.3</b> Family</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building.html"><a href="model-building.html#assumed-model-fitted-model"><i class="fa fa-check"></i><b>6.1.4</b> Assumed Model, Fitted Model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building.html"><a href="model-building.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>6.2</b> Explanation versus Prediction</a><ul>
<li class="chapter" data-level="6.2.1" data-path="model-building.html"><a href="model-building.html#explanation"><i class="fa fa-check"></i><b>6.2.1</b> Explanation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building.html"><a href="model-building.html#prediction"><i class="fa fa-check"></i><b>6.2.2</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building.html"><a href="model-building.html#summary"><i class="fa fa-check"></i><b>6.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html"><i class="fa fa-check"></i><b>7</b> Categorical Predictors and Interactions</a><ul>
<li class="chapter" data-level="7.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#dummy-variables"><i class="fa fa-check"></i><b>7.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="7.2" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#interactions"><i class="fa fa-check"></i><b>7.2</b> Interactions</a></li>
<li class="chapter" data-level="7.3" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#factor-variables"><i class="fa fa-check"></i><b>7.3</b> Factor Variables</a><ul>
<li class="chapter" data-level="7.3.1" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#factors-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.3.1</b> Factors with More Than Two Levels</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#parameterization"><i class="fa fa-check"></i><b>7.4</b> Parameterization</a></li>
<li class="chapter" data-level="7.5" data-path="categorical-predictors-and-interactions.html"><a href="categorical-predictors-and-interactions.html#building-larger-models"><i class="fa fa-check"></i><b>7.5</b> Building Larger Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Model Diagnostics</a><ul>
<li class="chapter" data-level="8.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#model-assumptions"><i class="fa fa-check"></i><b>8.1</b> Model Assumptions</a></li>
<li class="chapter" data-level="8.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#checking-assumptions"><i class="fa fa-check"></i><b>8.2</b> Checking Assumptions</a><ul>
<li class="chapter" data-level="8.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#fitted-versus-residuals-plot"><i class="fa fa-check"></i><b>8.2.1</b> Fitted versus Residuals Plot</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#breusch-pagan-test"><i class="fa fa-check"></i><b>8.2.2</b> Breusch-Pagan Test</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#histograms-1"><i class="fa fa-check"></i><b>8.2.3</b> Histograms</a></li>
<li class="chapter" data-level="8.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#q-q-plots"><i class="fa fa-check"></i><b>8.2.4</b> Q-Q Plots</a></li>
<li class="chapter" data-level="8.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#shapiro-wilk-test"><i class="fa fa-check"></i><b>8.2.5</b> Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#unusual-observations"><i class="fa fa-check"></i><b>8.3</b> Unusual Observations</a><ul>
<li class="chapter" data-level="8.3.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#leverage"><i class="fa fa-check"></i><b>8.3.1</b> Leverage</a></li>
<li class="chapter" data-level="8.3.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#outliers"><i class="fa fa-check"></i><b>8.3.2</b> Outliers</a></li>
<li class="chapter" data-level="8.3.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#influence"><i class="fa fa-check"></i><b>8.3.3</b> Influence</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#data-analysis-examples"><i class="fa fa-check"></i><b>8.4</b> Data Analysis Examples</a><ul>
<li class="chapter" data-level="8.4.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#good-diagnostics"><i class="fa fa-check"></i><b>8.4.1</b> Good Diagnostics</a></li>
<li class="chapter" data-level="8.4.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#suspect-diagnostics"><i class="fa fa-check"></i><b>8.4.2</b> Suspect Diagnostics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>9</b> Transformations</a><ul>
<li class="chapter" data-level="9.1" data-path="transformations.html"><a href="transformations.html#response-transformation"><i class="fa fa-check"></i><b>9.1</b> Response Transformation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="transformations.html"><a href="transformations.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>9.1.1</b> Variance Stabilizing Transformations</a></li>
<li class="chapter" data-level="9.1.2" data-path="transformations.html"><a href="transformations.html#box-cox-transformations"><i class="fa fa-check"></i><b>9.1.2</b> Box-Cox Transformations</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="transformations.html"><a href="transformations.html#predictor-transformation"><i class="fa fa-check"></i><b>9.2</b> Predictor Transformation</a><ul>
<li class="chapter" data-level="9.2.1" data-path="transformations.html"><a href="transformations.html#polynomials"><i class="fa fa-check"></i><b>9.2.1</b> Polynomials</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>10</b> Collinearity</a><ul>
<li class="chapter" data-level="10.1" data-path="collinearity.html"><a href="collinearity.html#exact-collinearity"><i class="fa fa-check"></i><b>10.1</b> Exact Collinearity</a></li>
<li class="chapter" data-level="10.2" data-path="collinearity.html"><a href="collinearity.html#collinearity-1"><i class="fa fa-check"></i><b>10.2</b> Collinearity</a><ul>
<li class="chapter" data-level="10.2.1" data-path="collinearity.html"><a href="collinearity.html#variance-inflation-factor."><i class="fa fa-check"></i><b>10.2.1</b> Variance Inflation Factor.</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="collinearity.html"><a href="collinearity.html#simulation-2"><i class="fa fa-check"></i><b>10.3</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>11</b> Model Selection</a><ul>
<li class="chapter" data-level="11.1" data-path="model-selection.html"><a href="model-selection.html#quality-criterion"><i class="fa fa-check"></i><b>11.1</b> Quality Criterion</a><ul>
<li class="chapter" data-level="11.1.1" data-path="model-selection.html"><a href="model-selection.html#akaike-information-criterion"><i class="fa fa-check"></i><b>11.1.1</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="11.1.2" data-path="model-selection.html"><a href="model-selection.html#bayesian-information-criterion"><i class="fa fa-check"></i><b>11.1.2</b> Bayesian Information Criterion</a></li>
<li class="chapter" data-level="11.1.3" data-path="model-selection.html"><a href="model-selection.html#adjusted-r-squared"><i class="fa fa-check"></i><b>11.1.3</b> Adjusted R-Squared</a></li>
<li class="chapter" data-level="11.1.4" data-path="model-selection.html"><a href="model-selection.html#cross-validated-rmse"><i class="fa fa-check"></i><b>11.1.4</b> Cross-Validated RMSE</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="model-selection.html"><a href="model-selection.html#selection-procedures"><i class="fa fa-check"></i><b>11.2</b> Selection Procedures</a><ul>
<li class="chapter" data-level="11.2.1" data-path="model-selection.html"><a href="model-selection.html#backward-search"><i class="fa fa-check"></i><b>11.2.1</b> Backward Search</a></li>
<li class="chapter" data-level="11.2.2" data-path="model-selection.html"><a href="model-selection.html#forward-search"><i class="fa fa-check"></i><b>11.2.2</b> Forward Search</a></li>
<li class="chapter" data-level="11.2.3" data-path="model-selection.html"><a href="model-selection.html#stepwise-search"><i class="fa fa-check"></i><b>11.2.3</b> Stepwise Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="model-selection.html"><a href="model-selection.html#exhaustive-search"><i class="fa fa-check"></i><b>11.2.4</b> Exhaustive Search</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="model-selection.html"><a href="model-selection.html#higher-order-terms"><i class="fa fa-check"></i><b>11.3</b> Higher Order Terms</a></li>
<li class="chapter" data-level="11.4" data-path="model-selection.html"><a href="model-selection.html#explanation-versus-prediction-1"><i class="fa fa-check"></i><b>11.4</b> Explanation versus Prediction</a><ul>
<li class="chapter" data-level="11.4.1" data-path="model-selection.html"><a href="model-selection.html#explanation-1"><i class="fa fa-check"></i><b>11.4.1</b> Explanation</a></li>
<li class="chapter" data-level="11.4.2" data-path="model-selection.html"><a href="model-selection.html#prediction-1"><i class="fa fa-check"></i><b>11.4.2</b> Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="beyond.html"><a href="beyond.html"><i class="fa fa-check"></i><b>12</b> Beyond</a><ul>
<li class="chapter" data-level="12.1" data-path="beyond.html"><a href="beyond.html#whats-next"><i class="fa fa-check"></i><b>12.1</b> What’s Next</a></li>
<li class="chapter" data-level="12.2" data-path="beyond.html"><a href="beyond.html#further-r-resources"><i class="fa fa-check"></i><b>12.2</b> Further <code>R</code> Resources</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>13</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="13.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#experiments"><i class="fa fa-check"></i><b>13.1</b> Experiments</a></li>
<li class="chapter" data-level="13.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#two-sample-t-test"><i class="fa fa-check"></i><b>13.2</b> Two-Sample t-Test</a></li>
<li class="chapter" data-level="13.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#one-way-anova"><i class="fa fa-check"></i><b>13.3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="13.3.1" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#factor-variables-1"><i class="fa fa-check"></i><b>13.3.1</b> Factor Variables</a></li>
<li class="chapter" data-level="13.3.2" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#some-simulation"><i class="fa fa-check"></i><b>13.3.2</b> Some Simulation</a></li>
<li class="chapter" data-level="13.3.3" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#power"><i class="fa fa-check"></i><b>13.3.3</b> Power</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#post-hoc-testing"><i class="fa fa-check"></i><b>13.4</b> Post Hoc Testing</a></li>
<li class="chapter" data-level="13.5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html#two-way-anova"><i class="fa fa-check"></i><b>13.5</b> Two-Way ANOVA</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/daviddalpiaz/appliedstats" target="blank">&copy; 2016 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Statistics with <code>R</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="collinearity" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Collinearity</h1>
<blockquote>
<p>“If I look confused it is because I am thinking.”</p>
<p>— <strong>Samuel Goldwyn</strong></p>
</blockquote>
<p>After reading this chapter you will be able to:</p>
<ul>
<li>Identify collinearity in regression.</li>
<li>Understand the effect of collinearity on regression models.</li>
</ul>
<div id="exact-collinearity" class="section level2">
<h2><span class="header-section-number">10.1</span> Exact Collinearity</h2>
<p>Let’s create a dataset where one of the predictors, <span class="math inline">\(x_3\)</span>, is a linear combination of the other predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gen_exact_collin_data =<span class="st"> </span>function(<span class="dt">num_samples =</span> <span class="dv">100</span>) {
  x1 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> num_samples, <span class="dt">mean =</span> <span class="dv">80</span>, <span class="dt">sd =</span> <span class="dv">10</span>)
  x2 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> num_samples, <span class="dt">mean =</span> <span class="dv">70</span>, <span class="dt">sd =</span> <span class="dv">5</span>)
  x3 =<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>x1 +<span class="st"> </span><span class="dv">4</span> *<span class="st"> </span>x2 +<span class="st"> </span><span class="dv">3</span>
  y =<span class="st"> </span><span class="dv">3</span> +<span class="st"> </span>x1 +<span class="st"> </span>x2 +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> num_samples, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
  <span class="kw">data.frame</span>(y, x1, x2, x3)
}</code></pre></div>
<p>Notice that the way we are generating this data, the response <span class="math inline">\(y\)</span> only really depends on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
exact_collin_data =<span class="st"> </span><span class="kw">gen_exact_collin_data</span>()
<span class="kw">head</span>(exact_collin_data)</code></pre></div>
<pre><code>##          y       x1       x2       x3
## 1 170.7135 93.70958 76.00483 494.4385
## 2 152.9106 74.35302 75.22376 452.6011
## 3 152.7866 83.63128 64.98396 430.1984
## 4 170.6306 86.32863 79.24241 492.6269
## 5 152.3320 84.04268 66.66613 437.7499
## 6 151.3155 78.93875 70.52757 442.9878</code></pre>
<p>What happens when we attempt to fit a regression model in <code>R</code> using all of the predictors?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">exact_collin_fit =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x1 +<span class="st"> </span>x2 +<span class="st"> </span>x3, <span class="dt">data =</span> exact_collin_data)
<span class="kw">summary</span>(exact_collin_fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2 + x3, data = exact_collin_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.57662 -0.66188 -0.08253  0.63706  2.52057 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 2.957336   1.735165   1.704   0.0915 .  
## x1          0.985629   0.009788 100.702   &lt;2e-16 ***
## x2          1.017059   0.022545  45.112   &lt;2e-16 ***
## x3                NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.014 on 97 degrees of freedom
## Multiple R-squared:  0.9923, Adjusted R-squared:  0.9921 
## F-statistic:  6236 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We see that <code>R</code> simply decides to exclude a variable. Why is this happening?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">as.matrix</span>(exact_collin_data[,-<span class="dv">1</span>]))
<span class="kw">solve</span>(<span class="kw">t</span>(X) %*%<span class="st"> </span>X)</code></pre></div>
<p>If we attempt to find <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> using <span class="math inline">\(\left( \boldsymbol{X}^T \boldsymbol{X} \right)^{-1}\)</span>, we see that this is not possible, due to the fact that the columns of <span class="math inline">\(\boldsymbol{X}\)</span> are linearly dependent. The previous lines of code were not run, because they produce an error!</p>
<p>When this happens, we say there is <strong>exact collinearity</strong> in the dataset.</p>
<p>As a result of this issue, <code>R</code> essentially chose to fit the model <code>y ~ x1 + x2</code>. However notice that two other models would accomplish exactly the same fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit1 =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x1 +<span class="st"> </span>x2, <span class="dt">data =</span> exact_collin_data)
fit2 =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x1 +<span class="st"> </span>x3, <span class="dt">data =</span> exact_collin_data)
fit3 =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x2 +<span class="st"> </span>x3, <span class="dt">data =</span> exact_collin_data)</code></pre></div>
<p>We see that the fitted values for each of the three models are exactly the same. This is a result of <span class="math inline">\(x_3\)</span> containing all of the information from <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. As long as one of <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span> are included in the model, <span class="math inline">\(x_3\)</span> can be used to recover the information from the variable not included.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">all.equal</span>(<span class="kw">fitted</span>(fit1), <span class="kw">fitted</span>(fit2))</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">all.equal</span>(<span class="kw">fitted</span>(fit2), <span class="kw">fitted</span>(fit3))</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>While their fitted values are all the same, their estimated coefficients are wildly different. The sign of <span class="math inline">\(x_2\)</span> is switched in two of the models! So only <code>fit1</code> properly <em>explains</em> the relationship between the variables, <code>fit2</code> and <code>fit3</code> still <em>predict</em> as well as <code>fit1</code>, despite the coefficients having little to no meaning, a concept we will return to later.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit1)</code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   2.9573357   0.9856291   1.0170586</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit2)</code></pre></div>
<pre><code>## (Intercept)          x1          x3 
##   2.1945418   0.4770998   0.2542647</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit3)</code></pre></div>
<pre><code>## (Intercept)          x2          x3 
##   1.4788921  -0.9541995   0.4928145</code></pre>
</div>
<div id="collinearity-1" class="section level2">
<h2><span class="header-section-number">10.2</span> Collinearity</h2>
<p>Exact collinearity is an extreme example of <strong>collinearity</strong>, which occurs in multiple regression when predictor variables are highly correlated. Collinearity is often called <em>multicollinearity</em>, since it is a phenomenon that really only occurs during multiple regression.</p>
<p>Looking at the <code>seatpos</code> dataset from the <code>faraway</code> package, we will see an example of this concept. The predictors in this dataset are various attributes of car drivers, such as their height, weight and age. The response variable <code>hipcenter</code> measures the “horizontal distance of the midpoint of the hips from a fixed location in the car in mm.” Essentially, it measures the position of the seat for a given driver. This is potentially useful information for car manufacturers considering comfort and safety when designing vehicles.</p>
<p>We will attempt to fit a model that predicts <code>hipcenter</code>. Two predictor variables are immediately interesting to us: <code>HtShoes</code> and <code>Ht</code>. We certainly expect a person’s height to be highly correlated to their height when wearing shoes. We’ll pay special attention to these two variables when fitting models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(faraway)
<span class="kw">pairs</span>(seatpos, <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-446-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">cor</span>(seatpos), <span class="dv">2</span>)</code></pre></div>
<pre><code>##             Age Weight HtShoes    Ht Seated   Arm Thigh   Leg hipcenter
## Age        1.00   0.08   -0.08 -0.09  -0.17  0.36  0.09 -0.04      0.21
## Weight     0.08   1.00    0.83  0.83   0.78  0.70  0.57  0.78     -0.64
## HtShoes   -0.08   0.83    1.00  1.00   0.93  0.75  0.72  0.91     -0.80
## Ht        -0.09   0.83    1.00  1.00   0.93  0.75  0.73  0.91     -0.80
## Seated    -0.17   0.78    0.93  0.93   1.00  0.63  0.61  0.81     -0.73
## Arm        0.36   0.70    0.75  0.75   0.63  1.00  0.67  0.75     -0.59
## Thigh      0.09   0.57    0.72  0.73   0.61  0.67  1.00  0.65     -0.59
## Leg       -0.04   0.78    0.91  0.91   0.81  0.75  0.65  1.00     -0.79
## hipcenter  0.21  -0.64   -0.80 -0.80  -0.73 -0.59 -0.59 -0.79      1.00</code></pre>
<p>After loading the <code>faraway</code> package, we do some quick checks of correlation between the predictors. Visually, we can do this with the <code>pairs()</code> function, which plots all possible scatterplots between pairs of variables in the dataset.</p>
<p>We can also do this numerically with the <code>cor()</code> function, which when applied to a dataset, returns all pairwise correlations. Notice this is a symmetric matrix. Recall that correlation measures strength and direction of the linear relationship between to variables. The correlation between <code>Ht</code> and <code>HtShoes</code> is extremely high. So high, that rounded to two decimal places, it appears to be 1!</p>
<p>Unlike exact collinearity, here we can still fit a model with all of the predictors, but what effect does this have?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hip_model =<span class="st"> </span><span class="kw">lm</span>(hipcenter ~<span class="st"> </span>., <span class="dt">data =</span> seatpos)
<span class="kw">summary</span>(hip_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hipcenter ~ ., data = seatpos)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -73.827 -22.833  -3.678  25.017  62.337 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 436.43213  166.57162   2.620   0.0138 *
## Age           0.77572    0.57033   1.360   0.1843  
## Weight        0.02631    0.33097   0.080   0.9372  
## HtShoes      -2.69241    9.75304  -0.276   0.7845  
## Ht            0.60134   10.12987   0.059   0.9531  
## Seated        0.53375    3.76189   0.142   0.8882  
## Arm          -1.32807    3.90020  -0.341   0.7359  
## Thigh        -1.14312    2.66002  -0.430   0.6706  
## Leg          -6.43905    4.71386  -1.366   0.1824  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 37.72 on 29 degrees of freedom
## Multiple R-squared:  0.6866, Adjusted R-squared:  0.6001 
## F-statistic:  7.94 on 8 and 29 DF,  p-value: 0.00001306</code></pre>
<p>One of the first things we should notice is that the <span class="math inline">\(F\)</span>-test for the regression tells us that the regression is significant, however each individual predictor is not. Another interesting result is the opposite signs of the coefficients for <code>Ht</code> and <code>HtShoes</code>. This should seem rather counter-intuitive. Increasing <code>Ht</code> increases <code>hipcenter</code>, but increasing <code>HtShoes</code> decreases <code>hipcenter</code>?</p>
<p>This happens as a result of the predictors being highly correlated. For example, the <code>HtShoe</code> variable explains a large amount of the variation in <code>Ht</code>. When they are both in the model, their effects on the response are lessened individually, but together they still explain a large portion of the variation of <code>hipcenter</code>.</p>
<p>We define <span class="math inline">\(R_j^2\)</span> to be the proportion of observed variation in the <span class="math inline">\(j\)</span>-th predictor explained by the other predictors. In other words <span class="math inline">\(R_j^2\)</span> is the multiple R-Squared for the regression of <span class="math inline">\(x_j\)</span> on each of the other predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ht_shoes_model =<span class="st"> </span><span class="kw">lm</span>(HtShoes ~<span class="st"> </span>. -<span class="st"> </span>hipcenter, <span class="dt">data =</span> seatpos)
<span class="kw">summary</span>(ht_shoes_model)$r.squared</code></pre></div>
<pre><code>## [1] 0.9967472</code></pre>
<p>Here we see that the other predictors explain <span class="math inline">\(99.67\%\)</span> of the variation in <code>HtShoe</code>. When fitting this model, we removed <code>hipcenter</code> since it is not a predictor.</p>
<div id="variance-inflation-factor." class="section level3">
<h3><span class="header-section-number">10.2.1</span> Variance Inflation Factor.</h3>
<p>Now note that the variance of <span class="math inline">\(\hat{\beta_j}\)</span> can be written as</p>
<p><span class="math display">\[
  Var(\hat{\beta_j}) = \sigma^2 C_{jj} = \sigma^2 \left( \frac{1}{1 - R_j^2}  \right) \frac{1}{S_{x_j x_j}}
\]</span></p>
<p>where <span class="math inline">\(S_{x_j x_j} = \sum(x_{ij}-\bar{x}_j)^2\)</span>. This gives us a way to understand how collinearity affects our regression estimates.</p>
<p>We will call,</p>
<p><span class="math display">\[
  \frac{1}{1 - R_j^2}
\]</span></p>
<p>the <strong>variance inflation factor.</strong> The variance inflation factor quantifies the effect of collinearity on the variance of our regression estimates. When <span class="math inline">\(R_j^2\)</span> is large, that is close to 1, <span class="math inline">\(x_j\)</span> is well explained by the other predictors. With a large <span class="math inline">\(R_j^2\)</span> the variance inflation factor becomes large. This tells us that when <span class="math inline">\(x_j\)</span> is highly correlated with other predictors, our estimate of <span class="math inline">\(\beta_j\)</span> is highly variable.</p>
<p>The <code>vif</code> function from the <code>faraway</code> package calculates the VIFs for each of the predictors of a model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(hip_model)</code></pre></div>
<pre><code>##        Age     Weight    HtShoes         Ht     Seated        Arm      Thigh 
##   1.997931   3.647030 307.429378 333.137832   8.951054   4.496368   2.762886 
##        Leg 
##   6.694291</code></pre>
<p>In practice it is common to say that any VIF greater than <span class="math inline">\(5\)</span> is cause for concern. So in this example we see there is a huge multicollinearity issue as many of the predictors have a VIF greater than 5.</p>
<p>Let’s further investigate how the presence of collinearity actually effects a model. If we add a small amount of noise to the data, we see that the estimates of the coefficients change drastically. This is a rather undesirable effect. Adding random noise should not effect the coefficients of a model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hip_model_noise =<span class="st"> </span><span class="kw">lm</span>(hipcenter +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">38</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">10</span>) ~<span class="st"> </span>., <span class="dt">data =</span> seatpos)</code></pre></div>
<p>Adding the noise had such a large effect, the sign of the coefficient for <code>Ht</code> has changed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hip_model</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hipcenter ~ ., data = seatpos)
## 
## Coefficients:
## (Intercept)          Age       Weight      HtShoes           Ht       Seated  
##   436.43213      0.77572      0.02631     -2.69241      0.60134      0.53375  
##         Arm        Thigh          Leg  
##    -1.32807     -1.14312     -6.43905</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hip_model_noise</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hipcenter + rnorm(38, mean = 0, sd = 10) ~ ., data = seatpos)
## 
## Coefficients:
## (Intercept)          Age       Weight      HtShoes           Ht       Seated  
##   462.93357      0.63646      0.01359     -2.13828     -0.13234      0.04610  
##         Arm        Thigh          Leg  
##    -0.20175     -1.40641     -5.72109</code></pre>
<p>This tells us that a model with collinearity is bad at explaining the relationship between the response and the predictors. We cannot even be confident in the direction of the relationship. However, does collinearity effect prediction?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">fitted</span>(hip_model), <span class="kw">fitted</span>(hip_model_noise), <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Predicted, Without Noise&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Predicted, With Noise&quot;</span>)
<span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-452-1.png" width="672" /></p>
<p>We see that by plotting the predicted values using both models against each other, they are actually rather similar.</p>
<p>Let’s now look at a smaller model,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hip_model_small =<span class="st"> </span><span class="kw">lm</span>(hipcenter ~<span class="st"> </span>Age +<span class="st"> </span>Weight +<span class="st"> </span>Ht, <span class="dt">data =</span> seatpos)
<span class="kw">summary</span>(hip_model_small)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = hipcenter ~ Age + Weight + Ht, data = seatpos)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -91.526 -23.005   2.164  24.950  53.982 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 528.297729 135.312947   3.904 0.000426 ***
## Age           0.519504   0.408039   1.273 0.211593    
## Weight        0.004271   0.311720   0.014 0.989149    
## Ht           -4.211905   0.999056  -4.216 0.000174 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 36.49 on 34 degrees of freedom
## Multiple R-squared:  0.6562, Adjusted R-squared:  0.6258 
## F-statistic: 21.63 on 3 and 34 DF,  p-value: 0.00000005125</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(hip_model_small)</code></pre></div>
<pre><code>##      Age   Weight       Ht 
## 1.093018 3.457681 3.463303</code></pre>
<p>Immediately we see that multicollinearity isn’t an issue here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(hip_model_small, hip_model)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: hipcenter ~ Age + Weight + Ht
## Model 2: hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + 
##     Leg
##   Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)
## 1     34 45262                           
## 2     29 41262  5    4000.3 0.5623 0.7279</code></pre>
<p>Also notice that using an <span class="math inline">\(F\)</span>-test to compare the two models, we would prefer the smaller model.</p>
<p>We now investigate the effect of adding another variable to this smaller model. Specifically we want to look at adding the variable <code>HtShoes</code>. So now our possible predictors are <code>HtShoes</code>, <code>Age</code>, <code>Weight</code>, and <code>Ht</code>. Our response is still <code>hipcenter</code>.</p>
<p>To quantify this effect we will look at a <strong>variable added plot</strong> and a <strong>partial correlation coefficient</strong>. For both of these, we will look at the residuals of two models:</p>
<ul>
<li>Regressing the response (<code>hipcenter</code>) against all of the predictors except the predictor of interest (<code>HtShoes</code>).</li>
<li>Regressing the predictor of interest (<code>HtShoes</code>) against the other predictors (<code>Age</code>, <code>Weight</code>, and <code>Ht</code>).</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ht_shoes_model_small =<span class="st"> </span><span class="kw">lm</span>(HtShoes ~<span class="st"> </span>Age +<span class="st"> </span>Weight +<span class="st"> </span>Ht, <span class="dt">data =</span> seatpos)</code></pre></div>
<p>So now, the residuals of <code>hip_model_small</code> give us the variation of <code>hipcenter</code> that is <em>unexplained</em> by <code>Age</code>, <code>Weight</code>, and <code>Ht</code>. Similarly, the residuals of <code>ht_shoes_model_small</code> give us the variation of <code>HtShoes</code> unexplained by <code>Age</code>, <code>Weight</code>, and <code>Ht</code>.</p>
<p>The correlation of these two residuals gives us the <strong>partial correlation coefficient</strong> of <code>HtShoes</code> and <code>hipcenter</code> with the effects of <code>Age</code>, <code>Weight</code>, and <code>Ht</code> removed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="kw">resid</span>(ht_shoes_model_small), <span class="kw">resid</span>(hip_model_small))</code></pre></div>
<pre><code>## [1] -0.01650317</code></pre>
<p>Since this value is small, close to zero, it means that the variation of <code>hipcenter</code> that is unexplained by <code>Age</code>, <code>Weight</code>, and <code>Ht</code> shows very little correlation with the variation of <code>HtShoes</code> that is not explained by <code>Age</code>, <code>Weight</code>, and <code>Ht</code>. Thus adding <code>HtShoes</code> to the model would likely be of little benefit.</p>
<p>Similarly a <strong>variable added plot</strong> visualizes these residuals against each other. It is also helpful to regress the residuals of the response against the residuals of the predictor and add the regression line to the plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">resid</span>(hip_model_small) ~<span class="st"> </span><span class="kw">resid</span>(ht_shoes_model_small), <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Residuals, Added Predictor&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Residuals, Original Model&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">abline</span>(<span class="kw">lm</span>(<span class="kw">resid</span>(hip_model_small) ~<span class="st"> </span><span class="kw">resid</span>(ht_shoes_model_small)),
       <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-457-1.png" width="672" /></p>
<p>Here the variable added plot shows almost no linear relationship. This tells us that adding <code>HtShoes</code> to the model would probably not be worthwhile. Since its variation is largely explained by the other predictors, adding it to the model will not do much to improve the model. However it will increase the variation of the estimates and make the model much harder to interpret.</p>
<p>Had there been a strong linear relationship here, thus a large partial correlation coefficient, it would likely have been useful to add the additional predictor to the model.</p>
<p>This trade off is mostly true in general. As a model gets more predictors, errors will get smaller and its <em>prediction</em> will be better, but it will be harder to interpret. This is why, if we are interested in <em>explaining</em> the relationship between the predictors and the response, we often want a model that fits well, but with a small number of predictors with little correlation.</p>
<p>Next chapter we will learn about methods to find models that both fit well, but also have a small number of predictors. We will also discuss <em>overfitting</em>. Although, adding additional predictors will always make errors smaller, sometimes we will be “fitting the noise” and such a model will not generalize to additional observations well.</p>
</div>
</div>
<div id="simulation-2" class="section level2">
<h2><span class="header-section-number">10.3</span> Simulation</h2>
<p>Here we simulate examples data with and without collinearity. We will note the difference in the distribution of the estimates of the <span class="math inline">\(\beta\)</span> parameters, in particular their variance. However, we will also notice the similarity in their <span class="math inline">\(MSE\)</span>.</p>
<p>We will use the model,</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim N(\mu = 0, \sigma^2 = 25)\)</span> and the <span class="math inline">\(\beta\)</span> coefficients defined below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
beta_0 =<span class="st"> </span><span class="dv">7</span>
beta_1 =<span class="st"> </span><span class="dv">3</span>
beta_2 =<span class="st"> </span><span class="dv">4</span>
sigma  =<span class="st"> </span><span class="dv">5</span></code></pre></div>
<p>We will use a sample size of 10, and 2000 simulations for both situations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample_size =<span class="st"> </span><span class="dv">10</span>
num_sim     =<span class="st"> </span><span class="dv">2000</span></code></pre></div>
<p>We’ll first consider the situation with a collinearity issue, so we manually create the two predictor variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x1 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>)
x2 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">6</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">8</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x1)</code></pre></div>
<pre><code>## [1] 3.02765</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x2)</code></pre></div>
<pre><code>## [1] 3.02765</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(x1, x2)</code></pre></div>
<pre><code>## [1] 0.9393939</code></pre>
<p>Notice that they have extremely high correlation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">true_line    =<span class="st"> </span>beta_0 +<span class="st"> </span>beta_1 *<span class="st"> </span>x1 +<span class="st"> </span>beta_2 *<span class="st"> </span>x2
beta_hat_bad =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, num_sim, <span class="dv">3</span>)
mse_bad      =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, num_sim)</code></pre></div>
<p>We perform the simulation 2000 times, each time fitting a regression model, and storing the estimated coefficients and the MSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">for (s in <span class="dv">1</span>:num_sim) {
  y =<span class="st"> </span>true_line +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> sigma)
  reg_out =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x1 +<span class="st"> </span>x2)
  beta_hat_bad[s, ] =<span class="st"> </span><span class="kw">coef</span>(reg_out)
  mse_bad[s] =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">resid</span>(reg_out) ^<span class="st"> </span><span class="dv">2</span>)
}</code></pre></div>
<p>Now we move to the situation without a collinearity issue, so we again manually create the two predictor variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x1 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>)
x2 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">10</span>)</code></pre></div>
<p>Notice that the standard deviations of each are the same as before, however, now the correlation is extremely close to 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x1)</code></pre></div>
<pre><code>## [1] 3.02765</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(x2)</code></pre></div>
<pre><code>## [1] 3.02765</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(x1, x2)</code></pre></div>
<pre><code>## [1] 0.03030303</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">true_line     =<span class="st"> </span>beta_0 +<span class="st"> </span>beta_1 *<span class="st"> </span>x1 +<span class="st"> </span>beta_2 *<span class="st"> </span>x2
beta_hat_good =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, num_sim, <span class="dv">3</span>)
mse_good      =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, num_sim)</code></pre></div>
<p>We then perform simulations and store the same results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">for (s in <span class="dv">1</span>:num_sim) {
  y =<span class="st"> </span>true_line +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> sigma)
  reg_out =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x1 +<span class="st"> </span>x2)
  beta_hat_good[s, ] =<span class="st"> </span><span class="kw">coef</span>(reg_out)
  mse_good[s] =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">resid</span>(reg_out) ^<span class="st"> </span><span class="dv">2</span>)
}</code></pre></div>
<p>We’ll now investigate the differences.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">hist</span>(beta_hat_bad[, <span class="dv">2</span>],
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>,
     <span class="dt">border =</span> <span class="st">&quot;dodgerblue&quot;</span>,
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="st">&quot;Histogram of &quot;</span> *<span class="kw">hat</span>(beta)[<span class="dv">1</span>]*<span class="st"> &quot; with Collinearity&quot;</span>),
     <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>])
)
<span class="kw">hist</span>(beta_hat_good[, <span class="dv">2</span>],
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>,
     <span class="dt">border =</span> <span class="st">&quot;dodgerblue&quot;</span>,
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="st">&quot;Histogram of &quot;</span> *<span class="kw">hat</span>(beta)[<span class="dv">1</span>]*<span class="st"> &quot; without Collinearity&quot;</span>),
     <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>])
)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-468-1.png" width="768" /></p>
<p>First, for <span class="math inline">\(\beta_1\)</span>, which has a true value of <span class="math inline">\(3\)</span>, we see that both with and without collinearity, the simulated values are centered near <span class="math inline">\(3\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(beta_hat_bad[,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 2.969413</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(beta_hat_good[,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 2.985792</code></pre>
<p>The way the predictors were created, the <span class="math inline">\(S_{x_j x_j}\)</span> portion of the variance is the same for the predictors in both cases, but the variance is still much larger in the simulations performed with collinearity. The variance is so large in the collinear case, that sometimes the estimated coefficient for <span class="math inline">\(\beta_1\)</span> is negative!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(beta_hat_bad[,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 1.631838</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(beta_hat_good[,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] 0.5572147</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">hist</span>(beta_hat_bad[, <span class="dv">3</span>],
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>,
     <span class="dt">border =</span> <span class="st">&quot;dodgerblue&quot;</span>,
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="st">&quot;Histogram of &quot;</span> *<span class="kw">hat</span>(beta)[<span class="dv">2</span>]*<span class="st"> &quot; with Collinearity&quot;</span>),
     <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">2</span>])
)
<span class="kw">hist</span>(beta_hat_good[, <span class="dv">3</span>],
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>,
     <span class="dt">border =</span> <span class="st">&quot;dodgerblue&quot;</span>,
     <span class="dt">main =</span> <span class="kw">expression</span>(<span class="st">&quot;Histogram of &quot;</span> *<span class="kw">hat</span>(beta)[<span class="dv">2</span>]*<span class="st"> &quot; without Collinearity&quot;</span>),
     <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">2</span>])
)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-471-1.png" width="768" /></p>
<p>We see the same issues with <span class="math inline">\(\beta_2\)</span>. On average the estimates are correct, but the variance is again much larger with collinearity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(beta_hat_bad[,<span class="dv">3</span>])</code></pre></div>
<pre><code>## [1] 4.034139</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(beta_hat_good[,<span class="dv">3</span>])</code></pre></div>
<pre><code>## [1] 4.001728</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(beta_hat_bad[,<span class="dv">3</span>])</code></pre></div>
<pre><code>## [1] 1.640392</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(beta_hat_good[,<span class="dv">3</span>])</code></pre></div>
<pre><code>## [1] 0.5533393</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">hist</span>(mse_bad,
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>,
     <span class="dt">border =</span> <span class="st">&quot;dodgerblue&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;MSE, with Collinearity&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;MSE&quot;</span>
)
<span class="kw">hist</span>(mse_good,
     <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>,
     <span class="dt">border =</span> <span class="st">&quot;dodgerblue&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;MSE, without Collinearity&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;MSE&quot;</span>
)</code></pre></div>
<p><img src="applied_statistics_files/figure-html/unnamed-chunk-474-1.png" width="768" /></p>
<p>Interestingly, in both cases, the MSE is roughly the same on average. Again, this is because collinearity effects a model’s ability to <em>explain</em>, but not predict.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(mse_bad)</code></pre></div>
<pre><code>## [1] 17.68559</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(mse_good)</code></pre></div>
<pre><code>## [1] 17.98918</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="transformations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-selection.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/appliedstats/edit/master/collinearity.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
