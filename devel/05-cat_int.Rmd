# Categorical Predictors and Interactions

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(width = 80, scipen = 4)
```

> "The greatest value of a picture is when it forces us to notice what we never expected to see."
>
> --- **John Tukey**

After reading this chapter you will be able to:

- Include and interpret categorical variables in a linear regression model by way of dummy variables.
- Understand the implications of using a model with a categorical variable in two ways: levels serving as unique predictors versus levels serving as a comparison to a baseline.
- Construct and interpret linear regression models with interaction terms.
- Identify categorical variables in a data set and convert them into factor variables, if necessary, using R.

So far in each of our analyses, we have only used numeric variables as predictors. We have also only used *additive models*, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to *interact*. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in `R` usage.

## Dummy Variables

For this chapter, we will briefly use the built in dataset `mtcars` before returning to our `autompg` dataset that we created in the last chapter. The `mtcars` dataset is somewhat smaller, so we'll quickly take a look at the entire dataset.

```{r}
mtcars
```

We will be interested in three of the variables: `mpg `, `hp`, and `am`.

- `mpg`: fuel efficiency, in miles per gallon.
- `hp`: horsepower, in foot-pounds per second.
- `am`: transmission. Automatic or manual.

As we often do, we will start by plotting the data. We are interested in `mpg` as the response variable, and `hp` as a predictor.

```{r}
plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
```

We used a common `R` "trick" when plotting this data. The `am` variable takes two possible values; `0` for automatic transmission, and `1` for manual transmissions. `R` can use numbers to represent colors, however the color for `0` is white. So we take the `am` vector and add `1` to it. Then observations with automatic transmissions are now represented by `1`, which is black in `R`, and manual transmission are represented by `2`, which is red in `R`. (Note, we are only adding `1` inside the call to `plot()`, we are not actually modifying the values stored in `am`.)

We now fit the SLR model

\[
Y = \beta_0 + \beta_1 x_1 + \epsilon,
\]

where $Y$ is `mpg` and $x_1$ is `hp`. For notational brevity, we drop the index $i$ for observations.

```{r}
mpg_hp_slr = lm(mpg ~ hp, data = mtcars)
```

We then re-plot the data and add the fitted line to the plot.

```{r}
plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(mpg_hp_slr, lwd = 2, col = "green")
```

We should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, `am` as $x_2$.

Our new model is

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]

where $x_1$ and $Y$ remain the same, but now

\[
x_2 =
  \begin{cases}
   1 & \text{manual transmission} \\
   0       & \text{automatic transmission}
  \end{cases}.
\]

In this case, we call $x_2$ a **dummy variable**. A dummy variable is somewhat unfortunately named, as it is in no way "dumb". In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to "code" for a binary categorical variable. Let's see how this works.

First, note that `am` is already a dummy variable, since it uses the values `0` and `1` to represent automatic and manual transmissions. Often, a variable like `am` would store the character values `auto` and `man` and we would either have to convert these to `0` and `1`, or, as we will see later, `R` will take care of creating dummy variables for us.

So, to fit the above model, we do so like any other multiple regression model we have seen before.

```{r}
mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)
```

Briefly checking the output, we see that `R` has estimated the three $\beta$ parameters.

```{r}
mpg_hp_add
```

Since $x_2$ can only take values `0` and `1`, we can effectively write two different models, one for manual and one for automatic transmissions.

For automatic transmissions, that is $x_2 = 0$, we have,

\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]

Then for manual transmissions, that is $x_2 = 1$, we have,

\[
Y = (\beta_0 + \beta_2) + \beta_1 x_1 + \epsilon.
\]

Notice that these models share the same slope, $\beta_1$, but have different intercepts, differing by $\beta_2$. So the change in `mpg` is the same for both models, but on average `mpg` differs by $\beta_2$ between the two transmission types.

We'll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:

- $\hat{\beta}_0$ = `coef(mpg_hp_add)[1]` = `r coef(mpg_hp_add)[1]`
- $\hat{\beta}_1$ = `coef(mpg_hp_add)[2]` = `r coef(mpg_hp_add)[2]`
- $\hat{\beta}_2$ = `coef(mpg_hp_add)[3]` = `r coef(mpg_hp_add)[3]`

We can then combine these to calculate the estimated slope and intercepts.

```{r}
int_auto = coef(mpg_hp_add)[1]
int_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]

slope_auto = coef(mpg_hp_add)[2]
slope_manu = coef(mpg_hp_add)[2]
```

Re-plotting the data, we use these slopes and intercepts to add the "two" fitted models to the plot.

```{r}
plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto
abline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual
```

We notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.

They say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that $\beta_2$ is significant, but let's verify mathematically. Essentially we would like to test:

\[
H_0: \beta_2 = 0 \quad \text{vs} \quad H_1: \beta_2 \neq 0.
\]

This is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a $t$ or $F$ test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line ($H_0$) against a model that allows two lines ($H_1$).

To obtain the test statistic and p-value for the $t$-test, we would use

```{r}
summary(mpg_hp_add)$coef[3,]
```

To do the same for the $F$ test, we would use

```{r}
anova(mpg_hp_slr, mpg_hp_add)
```

Notice that these are indeed testing the same thing, as the p-values are exactly equal. (And the $F$ test statistic is the $t$ test statistic squared.)

Recapping some interpretations:

- $\hat{\beta}_0 = `r coef(mpg_hp_add)[1]`$ is the estimated average `mpg` for a car with an automatic transmission and **0** `hp`.
- $\hat{\beta}_0 + \hat{\beta}_2 = `r coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]`$ is the estimated average `mpg` for a car with a manual transmission and **0** `hp`.

- $\hat{\beta}_2 = `r coef(mpg_hp_add)[3]`$ is the estimated **difference** in average `mpg` for cars with manual transmissions as compared to those with automatic transmission, for **any** `hp`.
- $\hat{\beta}_1 = `r coef(mpg_hp_add)[2]`$ is the estimated change in average `mpg` for an increase in one `hp`, for **either** transmission types.

We should take special notice of those last two. In the model,

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]

we see $\beta_1$ is the average change in $Y$ for an increase in $x_1$, *no matter* the value of $x_2$. Also, $\beta_2$ is always the difference in the average of $Y$ for *any* value of $x_1$. These are two restrictions we won't always want, so we need a way to specify a more flexible model.

Here we restricted ourselves to a single numerical predictor $x_1$ and one dummy variable $x_2$. However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the "two lines" interpretation. But in general, we can think of a dummy variable as creating "two models," one for each category of a binary categorical variable.

## Interactions

To remove the "same slope" restriction, we will now discuss **interaction**. To illustrate this concept, we will return to the `autompg` dataset we created in the last chapter, with a few more modifications.

```{r}
# read data frame from the web
autompg = read.table(
  "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data",
  quote = "\"",
  comment.char = "",
  stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin", "name")
# remove missing data, which is stored as "?"
autompg = subset(autompg, autompg$hp != "?")
# remove the plymouth reliant, as it causes some issues
autompg = subset(autompg, autompg$name != "plymouth reliant")
# give the dataset row names, based on the engine, year and name
rownames(autompg) = paste(autompg$cyl, "cylinder", autompg$year, autompg$name)
# remove the variable for name, as will as origin
autompg = subset(autompg, select = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin"))
# change horsepower from character to numeric
autompg$hp = as.numeric(autompg$hp)
# create a dummary variable for foreign vs domestic cars. domestic = 1.
autompg$domestic = as.numeric(autompg$origin == 1)
# remove 3 and 5 cylinder cars (which are very rare.)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]
# the following line would verify the remaining cylinder possibilities are 4, 6, 8
#unique(autompg$cyl)
# change cyl to a factor variable
autompg$cyl = as.factor(autompg$cyl)
# change origin to a factor variable
autompg$origin = as.factor(autompg$origin)
```

```{r}
str(autompg)
```

We've removed cars with `3` and `5` cylinders , as well as created a new variable `domestic` which indicates whether or not a car was built in the United States. Removing the `3` and `5` cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable `domestic` takes the value `1` if the car was built in the United States, and `0` otherwise, which we will refer to as "foreign." (We are arbitrarily using the United States as the reference point here.) We have also made `cyl` and `origin` into factor variables, which we will discuss later.

We'll now be concerned with three variables: `mpg`, `disp`, and `domestic`. We will use `mpg` as the response. We can fit a model,

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]

where 

- $Y$ is `mpg`, the fuel efficiency in miles per gallon,
- $x_1$ is `disp`, the displacement in cubic inches,
- $x_2$ is `domestic` as described above, which is a dummy variable.

We will fit this model, extract the slope and intercept for the "two lines," plot the data and add the lines.

```{r}
mpg_disp_add = lm(mpg ~ disp + domestic, data = autompg)

int_for = coef(mpg_disp_add)[1]
int_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]

slope_for = coef(mpg_disp_add)[2]
slope_dom = coef(mpg_disp_add)[2]

plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars
```

This is a model that allows for two *parallel* lines, meaning the `mpg` can be different on average between foreign and domestic cars of the same engine displacement, but the change in average `mpg` for an increase in displacement is the same for both. We can see this model isn't doing very well here. The red line fits the red points fairly well, but the black line isn't doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.

Consider the following model,

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]

where $x_1$, $x_2$, and $Y$ are the same as before, but we have added a new **interaction** term $x_1 x_2$ which multiplies $x_1$ and $x_2$, so we also have an additional $\beta$ parameter $\beta_3$.

This model essentially creates two slopes and two intercepts, $\beta_2$ being the difference in intercepts and $\beta_3$ being the difference in slopes. To see this, we will break down the model into the two "sub-models" for foreign and domestic cars.

For foreign cars, that is $x_2 = 0$, we have

\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]

For domestic cars, that is $x_2 = 1$, we have

\[
Y = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) x_1 + \epsilon.
\]

These two models have both different slopes and intercepts. 

- $\beta_0$ is the average `mpg` for a foreign car with **0** `disp`.
- $\beta_1$ is the change in average `mpg` for an increase of one `disp`, for **foreign** cars.
- $\beta_0 + \beta_2$ is the average `mpg` for a domestic car with **0** `disp`.
- $\beta_1 + \beta_3$ is the change in average `mpg` for an increase of one `disp`, for **domestic** cars.

How do we fit this model in `R`? There are a number of ways.

One method would be to simply create a new variable, then fit a model like any other.

```{r, eval = FALSE}
autompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!
do_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!
```

You should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell `R` we would like to use the existing data with an interaction term, which it will create automatically when we use the `:` operator.

```{r}
mpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)
```

An alternative method, which will fit the exact same model as above would be to use the `*` operator. This method automatically creates the interaction term, as well as any "lower order terms," which in this case are the first order terms for `disp` and `domestic`

```{r}
mpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)
```

We can quickly verify that these are the doing the same thing.

```{r}
coef(mpg_disp_int)
coef(mpg_disp_int2)
```

We see that both the variables, and their coefficient estimates are indeed the same for both models.

```{r}
summary(mpg_disp_int)
```

We see that using `summary()` gives the usual output for a multiple regression model. We pay close attention to the line for `disp:domestic` which tests,

\[
H_0: \beta_3 = 0.
\]

In this case, testing for $\beta_3 = 0$ is testing for two lines with parallel slopes versus two lines with possibly different slopes. The `disp:domestic` line in the `summary()` output uses a $t$-test to perform the test.

We could also use an ANOVA $F$-test. The additive model, without interaction is our null model, and the interaction is the alternative.

```{r}
anova(mpg_disp_add, mpg_disp_int)
```

Again we see this test has the same p-value as the $t$-test. Also the p-value is extremely low, so between the two, we choose the interaction model.

```{r}
int_for = coef(mpg_disp_int)[1]
int_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]

slope_for = coef(mpg_disp_int)[2]
slope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]
```

Here we again calculate the slope and intercepts for the two lines for use in plotting.

```{r}
plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars
```

We see that these lines fit the data much better, which matches the result of our tests.

So far we have only seen interaction between a categorical variable (`domestic`) and a numerical variable (`disp`). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.

Consider the model,

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]

where

- $Y$ is `mpg`, the fuel efficiency in miles per gallon,
- $x_1$ is `disp`, the displacement in cubic inches,
- $x_2$ is `hp`, the horsepower, in foot-pounds per second.

How does `mpg` change based on `disp` in this model? We can rearrange some terms to see how.

\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon
\]

So, for a one unit increase in $x_1$ (`disp`), the mean of $Y$ (`mpg`) increases $\beta_1 + \beta_3 x_2$, which is a different value depending on the value of $x_2$ (`hp`)! 

Since we're now working in three dimensions, this model can't be easily justified like the previous. Instead, we will have to rely on a test.

```{r}
mpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)
mpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)
summary(mpg_disp_int_hp)
```

Using `summary()` we focus on the line for `disp:hp` which tests,

\[
H_0: \beta_3 = 0.
\]

Again, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. Again, there is an equivalent $F$-test.

```{r}
anova(mpg_disp_add_hp, mpg_disp_int_hp)
```

We can take a closer look at the coefficients of our fitted interaction model.

```{r}
coef(mpg_disp_int_hp)
```

- $\hat{\beta}_0 = `r coef(mpg_disp_int_hp)[1]`$ is the estimated average `mpg` for a car with 0 `disp` and 0 `hp`.
- $\hat{\beta}_1 = `r coef(mpg_disp_int_hp)[2]`$ is the estimated change in average `mpg` for an increase in 1 `disp`, **for a car with 0 `hp`**.
- $\hat{\beta}_2 = `r coef(mpg_disp_int_hp)[3]`$ is the estimated change in average `mpg` for an increase in 1 `hp`, **for a car with 0 `disp`**.
- $\hat{\beta}_3 = `r coef(mpg_disp_int_hp)[4]`$ is an estimate of the modification to the change in average `mpg` for an increase in `disp`, for a car of a certain `hp`. (Or vice versa.)

That last coefficient needs further explanation. Recall the rearrangement we made earlier

\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon.
\]

So, our estimate for $\beta_1 + \beta_3 x_2$, is $\hat{\beta}_1 + \hat{\beta}_3 x_2$, which in this case is

\[
`r coef(mpg_disp_int_hp)[2]` + `r coef(mpg_disp_int_hp)[4]` x_2.
\]

This says that, for an increase of one `disp` we see an estimated change in average `mpg` of $`r coef(mpg_disp_int_hp)[2]` + `r coef(mpg_disp_int_hp)[4]` x_2$. So how `disp` and `mpg` are related, depends on the `hp` of the car.

So for a car with 50 `hp`, the estimated change in average `mpg` for an increase of one `disp` is

\[
`r coef(mpg_disp_int_hp)[2]` + `r coef(mpg_disp_int_hp)[4]` 50 = `r coef(mpg_disp_int_hp)[2] + coef(mpg_disp_int_hp)[4] * 50`
\]

So for a car with 350 `hp`, the estimated change in average `mpg` for an increase of one `disp` is

\[
`r coef(mpg_disp_int_hp)[2]` + `r coef(mpg_disp_int_hp)[4]` 350 = `r coef(mpg_disp_int_hp)[2] + coef(mpg_disp_int_hp)[4] * 350`
\]

Notice the sign changed!

## Factor Variables

- from binary categorical to 3+ categories
    - Levels
- caution about "factor" variables that are actually numeric 1,2,3 but don't have meaning. like `cyl`

```{r}
is.factor(autompg$cyl)
levels(autompg$cyl)
```

- show an example with "characters"?

- binary variables can be factors. difference in coding/display.

```{r}
is.factor(autompg$domestic)
```

- coercion?
    - coerce domestic to be factor, fit, maybe discuss R reordering some things.

- fit additive vs interaction first, talk about `R`'s parameterization first.

- compare to other paramterizations.
- individual intercepts
- show same as "single" intercept
- 1 = v1 + v2 + v3 (why we can't do some stuff)

- talk about how the var * dum is really nice with multiple level factors, do "long way" to compare

- talk about REFERENCE level


```{r}
mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg)
mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg)

anova(mpg_disp_add_cyl, mpg_disp_int_cyl)

int_4cyl = coef(mpg_disp_add_cyl)[1]
int_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]
int_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]

slope_all_cyl = coef(mpg_disp_add_cyl)[2]

plot(mpg ~ disp, data = autompg, col = cyl)
abline(int_4cyl, slope_all_cyl, col = 1, lty = 1, lwd = 2)
abline(int_6cyl, slope_all_cyl, col = 2, lty = 2, lwd = 2)
abline(int_8cyl, slope_all_cyl, col = 3, lty = 3, lwd = 2)


# note 8 is in the middle!


int_4cyl = coef(mpg_disp_int_cyl)[1]
int_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]
int_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]

slope_4cyl = coef(mpg_disp_int_cyl)[2]
slope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]
slope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]

plot(mpg ~ disp, data = autompg, col = cyl)
abline(int_4cyl, slope_4cyl, col = 1, lty = 1, lwd = 2)
abline(int_6cyl, slope_6cyl, col = 2, lty = 2, lwd = 2)
abline(int_8cyl, slope_8cyl, col = 3, lty = 3, lwd = 2)

# note: now order is better, in the right ranges!
```


```{r}
all.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)), fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))
identical(fitted(lm(mpg ~ disp * cyl, data = autompg)), fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))
```

```{r}
lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)
lm(mpg ~ disp * cyl, data = autompg)
lm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)
```


\[
v_1 =
  \begin{cases}
   1 & \text{4 cylinder} \\
   0       & \text{not 4 cylinder}
  \end{cases}
\]

\[
v_2 =
  \begin{cases}
   1 & \text{6 cylinder} \\
   0       & \text{not 6 cylinder}
  \end{cases}
\]

\[
v_3 =
  \begin{cases}
   1 & \text{8 cylinder} \\
   0       & \text{not 8 cylinder}
  \end{cases}
\]






\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon.
\]


- $Y = \beta_0 + \beta_1 x + \epsilon$
- $Y = (\beta_0 + \beta_2) + \beta_1 x + \epsilon$
- $Y = (\beta_0 + \beta_3) + \beta_1 x + \epsilon$

Note:

\[
\boldsymbol{1} = v_1 + v_2 + v_3
\]


\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta x +\epsilon.
\]

- $Y = \mu_1 + \beta x + \epsilon$
- $Y = \mu_2 + \beta x + \epsilon$
- $Y = \mu_3 + \beta x + \epsilon$



\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon
\]

- $Y = \beta_0 + \beta_1 x + \epsilon$
- $Y = (\beta_0 + \beta_2) + (\beta_1 + \gamma_2) x + \epsilon$
- $Y = (\beta_0 + \beta_3) + (\beta_1 + \gamma_3) x + \epsilon$



\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta_1 x v_1 + \beta_2 x v_2 + \beta_3 x v_3 +\epsilon
\]

- $Y = \mu_1 + \beta_1 x + \epsilon$
- $Y = \mu_2 + \beta_2 x + \epsilon$
- $Y = \mu_3 + \beta_3 x + \epsilon$








## Building Larger Models

fit a "big" model and do some interpretation

multiple-way interaction
tests about bigger model.

hierarchy

```{r}
summary(lm(mpg ~ disp * hp * domestic, data = autompg))
```


- test 3 way interaction, vs not
- test all other interactions, vs not

## REMOVE THIS SECTION

TODO: remove setup
