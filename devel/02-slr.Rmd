# Simple Linear Regression

> "All models are wrong, but some are useful."
>
> --- **George E. P. Box**

After reading this chapter you will be able to:

- ... . 
- ... .
- ... .
- ... .
- ... .

## Modeling

Let's consider a simple example of how the speed a car is traveling affects its stopping distance. (How far it travels before it comes to a stop.) To examine this relationship, we will use the `cars` dataset which, is a deafult `R` dataset.

To get a first look at the data you can use the `View()` function inside RStudio.

```{r, eval = FALSE}
View(cars)
```

We could also take a look at the variables names, the dimension of the data frame, and some sample observations with `str().`

```{r}
str(cars)
```

As we have seen before with data frames, there are a number of additional functions to access some of this information directly.

```{r}
dim(cars)
nrow(cars)
ncol(cars)
```

Other than the two variable names and the number of observations, this data is still just a bunch of numbers, so we should probably obtain some context.

```{r, eval = FALSE}
?cars
```

Reading the documentation we learn that this is data gathered during the 1920s about the speed of cars, and the resulting distance it takes for the car to come to a stop. The interesting task here is to determine how far a car travels before stopping, when traveling at a certain speed. So, we will first plot the stopping distance against the speed.

```{r}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 3,
     col  = "dodgerblue")
```

Let's now define some terminology. We have pairs of data, $(x_i, y_i)$, for $i = 1, 2, \ldots n$, where $n$ is the sample size of the dataset.

We use $i$ as an index, simply for notation. We use $x_i$ as the **predictor** (explanatory) variable. The predictor variable is used to help *predict* or explain the **response** (target, outcome) variable, $y_i$. 

In the `cars` explain, we are interested in using the predictor variable `speed` to predict and explain the response variable `dist`.

TODO: delete next line?, save for video

Sometimes you will see "independent variable" used in place of predictor variable. Also "dependent variable" in place of the response variable. While these are not incorrect, independence is a already a strictly defined concept in probability, so we prefer to stay away from this nomenclature.

Broadly speaking, we would like to model the relationship between $X$ and $Y$ using the form

\[
Y = f(X) + \epsilon.
\]

The function $f$ describes the functional relationship between the two variables, and the $\epsilon$ term is used as an error term. You could think of this a number of ways:

- Response = Prediction + Error
- Response = Signal + Noise
- Response = Model + Unexplained

TODO: move unexplained source to explaination vs prediction?

Unexplained sources:

- missing variables
- measurement error

What sort of function should we use for $f(X)$ for the `cars` data?

We could try to model the data with a horizontal line. That is, the the model for $y$ does not depend on the value of $x$. (Some function $f(X) = c$.) Here, we see this doesn't seem to do a very good job. Many of the data points are very far from the line. This is an example of **underfitting**. The obvious fix is to use make the function $f(X)$ actually depend on $x$.

```{r underfit_plot, echo = FALSE}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 3,
     col  = "dodgerblue")
underfit_model = lm(dist ~ 1, data = cars)
abline(underfit_model, lwd = 3, col = "darkorange")
```

We could also try to model the data with a very "wiggly" function that tries to go through as many of the data points as possible. This also doesn't seem to work very well. The stopping distance for a speed of 5 mph shouldn't be off the chart! (Even in 1920.) This is an example of **overfitting**. (Note that in this example no function will go through every point, since there are some $x$ values that have several possible $y$ values in the data.)

```{r overfit_plot, echo = FALSE}
overfit_model = lm(dist ~ poly(speed, 18), data = cars)
x = seq(-10, 50, length.out = 200)
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 3,
     col  = "dodgerblue")
lines(x, predict(overfit_model, data.frame(speed = x)), lwd = 2, col = "darkorange")
```

Lastly, we could try to model the data with a well chosen line. This line seems to summarize the relationship between stopping distance and speed quite well. As speed increases, the distance required to come to a stop increases. There is still some variation about this line, but it seems to capture the overall trend.

```{r goodfit_plot, echo = FALSE}
stop_dist_model = lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 3,
     col  = "dodgerblue")
abline(stop_dist_model, lwd = 3, col = "darkorange")
```

With this in mind, we would like to restrict our choice of $f(X)$ to *linear* functions of $X$. We will write our model using $\beta_1$ for the slope, and $\beta_0$ for the intercept,

\[
Y = \beta_0 + \beta_1 X + \epsilon.
\]

### Simple Linear Regression Model

We now define what we will call the simple linear regression model,

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon
\]

where $\epsilon_i \sim N(0, \sigma^2)$. That is, the $\epsilon_i$ are *independent and identically distributed* (iid) normal random variables with mean $0$ and variance $\sigma^2$.

TODO: this is where we should discuss the difference between random and not. Y vs Y_i, note Y and X random, $(x_i, y_i)$ are observed data.

The $x_i$ are considered fixed, known constants. The model then has three parameters to be estimated: $\beta_0$, $\beta_1$, and $\sigma^2$.

TODO: add something about conditional distribution to Y here? mean and variance?
TODO: or remove/change the following?

\[
E[Y_i] = \beta_0 + \beta_1 x_i
\]

and

\[
Var[Y_i] = \sigma^2.
\]

![Simple Linear Regression Model [UC David Stat Wiki](http://statwiki.ucdavis.edu/Textbook_Maps/General_Statistics/Map%3A_Introductory_Statistics_(Shafer_and_Zhang)/10%3A_Correlation_and_Regression/10.3_Modelling_Linear_Relationships_with_Randomness_Present)](../images/model.jpg)

TODO: mean of Y is different for different values of x.
TODO: use Y when it is random?
TODO: make a note that $y_i$ is a data point. $Y_i$ is a random variable.
TODO: $x_i$ are fixed, known constants. (Technically and assumption.)

PARAMETERS

ASSUMPTIONS: **LINE**

- **L**inear
- **I**ndepedent
- **N**ormal
- **E**qual Variance

- SIMPLE: single explanatory
- LINEAR: yes it is a line in this case, but that's confusing and we'll clarify later
- REGRESSION: relationship between two variables


TODO: merge these next two sentences.

How to find line? Find one with "small errors." How should we define errors?

The questions now becomes, how do we find such a line? There are many approaches we could take.

We could find the line that has the smallest maximum distance from any of the points to the line. That is,

\[
\underset{\beta_0, \beta_1}{\mathrm{argmin}} \max|y_i - (\beta_0 + \beta_1 x_i)|.
\]

We could find the line that minimizes the sum of all the distances from the points to the line. That is,

\[
\underset{\beta_0, \beta_1}{\mathrm{argmin}} \sum_{i = 1}^{n}|y_i - (\beta_0 + \beta_1 x_i)|.
\]

We could find the line that minimizes the sum of all the squared distances from the points to the line. That is,

\[
\underset{\beta_0, \beta_1}{\mathrm{argmin}} \sum_{i = 1}^{n}(y_i - (\beta_0 + \beta_1 x_i))^2.
\]

This last option is called the method of **least squares**. It is essentially the de-facto method for fitting a line to data. (You may have even seen it before in a linear algebra course.) Its popularity is largely due to the fact that it is mathematically "easy." (Which was important historically, as computers are a modern contraption.) It is also very popular because many relationships are well approximated by a linear function.

##  Least Squares Approach

We want to find values of $\beta_0$ and $\beta_1$ which minimimize

\[
f(\beta_0, \beta_1) = \sum_{i = 1}^{n}(y_i - (\beta_0 + \beta_1 x_i))^2 = \sum_{i = 1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2.
\]

We will call these values $\hat{\beta}_0$ and $\hat{\beta}_1$.

First, we take a partial derivative with respect to both $\beta_0$ and $\beta_1$.

\[
\begin{aligned}
\frac{\partial f}{\partial \beta_0} &= -2 \sum_{i = 1}^{n}(y_i - \beta_0 - \beta_1 x_i) \\
\frac{\partial f}{\partial \beta_1} &= -2 \sum_{i = 1}^{n}(x_i)(y_i - \beta_0 - \beta_1 x_i)
\end{aligned}
\]

We then set each of the partial derivates equal to zero and solving the resulting system of equations.

\[
\begin{aligned}
\sum_{i = 1}^{n}(y_i - \beta_0 - \beta_1 x_i) &= 0 \\
\sum_{i = 1}^{n}(x_i)(y_i - \beta_0 - \beta_1 x_i) &= 0 
\end{aligned}
\]

While solving the system of equations, one common algebraic rearrangement results in the **normal equations**.

\[
\begin{aligned}
\sum_{i = 1}^{n} y_i &= n \beta_0 + \beta_1 \sum_{i = 1}^{n} x_i \\
\sum_{i = 1}^{n} x_i y_i &= \beta_0 \sum_{i = 1}^{n} x_i + \beta_1 \sum_{i = 1}^{n} x_i^2
\end{aligned}
\]

Finally, we finish solving the system of equations.

\[
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}}{\sum_{i = 1}^{n} x_i^2 - \frac{(\sum_{i = 1}^{n} x_i)^2}{n}} = \frac{S_{xy}}{S_{xx}}\\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}
\end{aligned}
\]

Here, we have defined some notation for the expression we've obtained. Note that they have alternative forms which are much easier to work with. (We won't do it here, but you can try to prove the equalities below on your own, for "fun.")

TODO: define notation further. S_xy = sum of squares.... link to james' page?

\[
\begin{aligned}
S_{xy} &= \sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}  = \sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})\\
S_{xx} &= \sum_{i = 1}^{n} x_i^2 - \frac{(\sum_{i = 1}^{n} x_i)^2}{n}  = \sum_{i = 1}^{n}(x_i - \bar{x})^2\\
S_{yy} &= \sum_{i = 1}^{n} y_i^2 - \frac{(\sum_{i = 1}^{n} y_i)^2}{n}  = \sum_{i = 1}^{n}(y_i - \bar{y})^2
\end{aligned}
\]

By using the above alternative expression, we arrive at a cleaner, more useful expression for $\hat{\beta}_1$.

\[
\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^{n}(x_i - \bar{x})^2}
\]

Traditionally we would now calculate $\hat{\beta}_0$ and $\hat{\beta}_1$ by hand for the `cars` dataset. However because we are living in the 21st century and are intellegent (or lazy or efficient, depending on your perspective) we will utilize `R` to do the number crunching for us.

To keep some notation consistent with above mathematics, we will store the response variable as `y` and the predictor variable as `x`.

```{r}
x = cars$speed
y = cars$dist
```

We then calculate the three sums of squares defined above.

```{r}
Sxy = sum((x - mean(x)) * (y - mean(y)))
Sxx = sum((x - mean(x)) ^ 2)
Syy = sum((y - mean(y)) ^ 2)
c(Sxy, Sxx, Syy)
```

Then finally calculate $\hat{\beta}_0$ and $\hat{\beta}_1$.

```{r}
beta_1_hat = Sxy / Sxx
beta_0_hat = mean(y) - beta_1_hat * mean(x)
c(beta_0_hat, beta_1_hat)
```

TODO: model is unknown true line.
TODO: Fitted line.

\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x
\]

In this case,

\[
\hat{y} = `r round(beta_0_hat, 3)` + `r round(beta_1_hat, 3)` x
\]

We can now use this line to make predictions.

```{r}
unique(cars$speed)
```

TODO: interpretation of params and estimates.

TODO: fitted value

\[
\hat{y} = `r round(beta_0_hat, 3)` + `r round(beta_1_hat, 3)` \times 8 = `r round(beta_0_hat + beta_1_hat * 8, 3)`
\]

```{r}
beta_0_hat + beta_1_hat * 8 #this one is in data set and only has one y value, useful later.
```

TODO: interpolate

\[
\hat{y} = `r round(beta_0_hat, 3)` + `r round(beta_1_hat, 3)` \times 21 = `r round(beta_0_hat + beta_1_hat * 21, 3)`
\]

```{r}
beta_0_hat + beta_1_hat * 21
```

TODO: extrapolation

\[
\hat{y} = `r round(beta_0_hat, 3)` + `r round(beta_1_hat, 3)` \times 50 = `r round(beta_0_hat + beta_1_hat * 50, 3)`
\]

```{r}
beta_0_hat + beta_1_hat * 50
```

```{r}
range(x)
```

Response = Prediction + Error

\[
y = \hat{y} + e
\]

We then define a **residual** to be the observed value minus the predicted value.

\[
e_i = y_i - \hat{y}_i
\]

residual = actual - predicted

```{r}
which(cars$speed == 8)
cars[5, ]
cars[which(cars$speed == 8), ]
```

\[
e = 16 - `r round(beta_0_hat + beta_1_hat * 8, 3)` = `r round(16 - (beta_0_hat + beta_1_hat * 8), 3)`
\]

```{r}
16 - (beta_0_hat + beta_1_hat * 8)
```

Recall that,

\[
E[Y_i] = \beta_0 + \beta_1 x_i.
\]

So,

\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]

is a natural estimate for the mean of $Y_i$ for a given value of $x_i$.

Recall that when we specified out model, we had three unknown parameters; $\beta_0$, $\beta_1$, and $\sigma^2$. The method of least squares gave us estimates for $\beta_0$ and $\beta_1$, however, we have yet to see an estimate for $\sigma^2$. We will now define $s_e^2$ which will be an estimate for $\sigma^2$.

\[
\begin{aligned}
s_e^2 &= \frac{1}{n - 2} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 \\
      &= \frac{1}{n - 2} \sum_{i = 1}^{n} e_i^2
\end{aligned}
\]

This probably seems like a natural estimate, aside from the use of $n - 2$, which we will put off until next chapter. (We will also define another estimate later in this chapter that uses $n$ instead.) It should actually look rather familiar.

\[
s^2 = \frac{1}{n - 1}\sum_{i=1}^{n}(x_i - \bar{x})^2
\]

Here $s^2$, which we have seen before, is the estiamte of $\sigma^2$ when we have a single random variable $X$. In this case $\bar{x}$ is an estimate of $\mu$ which is assumed to be the same for each $x$.

Now, in the regression case, with $s_e^2$, each $y$ has a different mean because of the realationship with $x$. Thus, for each $y_i$, we use a different estimate of the mean, that is $\hat{y}_i$.

```{r}
y_hat = beta_0_hat + beta_1_hat * x
e     = y - y_hat
n     = length(e)
s2_e  = sum(e^2) / (n - 2)
s2_e
```

##  Decomposition of Variation

\[
y_i - \bar{y} = (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y})
\]

- $y_i - \bar{y}$ Total Deviation
- $y_i - \hat{y}_i$ Deivation of fitted regression line about mean
- $\hat{y}_i - \bar{y}$ deviation about fitted regression line (error)

\[
\sum_{i=1}^{n}(y_i - \bar{y})^2 = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2.
\]

\[
SST = \sum_{i=1}^{n}(y_i - \bar{y})^2
\]

\[
SSReg = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
\]

\[
SSE = RSS = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2
\]

###  Coefficient of Determination

The **coefficient of determination**, $R^2$, is defined as

\[
R^2 = \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} = 
1 - \frac{\sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} = 
1 - \frac{\sum_{i = 1}^{n}e_i^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
\]

The coefficient of determination is interpreted as the proportion of observed variation in $y$ that can be explained by the simple linear regression model.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
## TODO: this code and be made much better
# show for different R^2 values?
#

set.seed(42)
generate_data <- function(int = 1,
                          slope = 2,
                          sigma = 5,
                          n_obs = 15,
                          x_min = 0,
                          x_max = 10) {
  x <- seq(x_min, x_max, length.out = n_obs)
  y <- int + slope * x + rnorm(n_obs, 0, sigma)
  fit <- lm(y ~ x)
  y_hat <- fitted(fit)
  y_bar <- rep(mean(y), n_obs)
  data.frame(x, y, y_hat, y_bar)
}

plot_total_dev <- function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SST (Sum of Squares Total)", 
       xlab = "x", ylab = "y", pch = 20, cex = 3, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 2, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev <- function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSE (Sum of Squares Error)",
       xlab = "x", ylab = "y", pch = 20, cex = 3, col = "grey")
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_exp_dev <- function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)", 
  xlab = "x", ylab = "y", pch = 20, cex = 3, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

TODO: equal spacing is by choice.
TODO: the following are for a low/medium and high R2 example

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot_data <- generate_data(sigma = 2)
summary(lm(y ~ x, data = plot_data))$r.sq
plot_exp_dev(plot_data)
plot_unexp_dev(plot_data)
plot_total_dev(plot_data)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plot_data <- generate_data(sigma = 6)
summary(lm(y ~ x, data = plot_data))$r.sq
plot_exp_dev(plot_data)
plot_unexp_dev(plot_data)
plot_total_dev(plot_data)
```

## The `lm` Function

So far we have done regression by deriving the least squares estimates, then writing simple `R` commands to perform the necessary calculations. Since this is such a common task, this is functionality that is built directly into `R` via the `lm()` command.

The `lm()` command is used to fit **linear models** which accounts for a broader class of models than simple linear regression, but we will use SLR as our first demonstration of `lm()`. The `lm()` function will be one of our most recently used tools, so you may want to take a look at the documentation by using `?lm`. You'll notice there is a lot of information there, but we will start with just the very basics.

We'll continue using the `cars` data, and essentially use the `lm()` function to check the work we had previously done.

```{r}
stop_dist_model = lm(dist ~ speed, data = cars)
```

This line of code fits our very first linear model. The syntax should look somewhat familiar. We use the `dist ~ speed` syntax to tell `R` we would like to model the response variable `dist` as a linear function of the predictor variable `speed`. In general you should thik of the syntax as `response ~ predictor`. The `data = cars` arguement then tells `R` that that `dist` and `speed` variables are from the dataset `cars`. We then store this result in a variable `stop_dist_model`.

The variable `stop_dist_model` now contains a wealth of informaiton and we will now see how to extract and use that informaiton. The first thing we will do is simply output whatever is stored immediately in the variable `stop_dist_model`.

```{r}
stop_dist_model
```

We see that it tells us the formula we input into `R`, that is `lm(formula = dist ~ speed, data = cars)`. We also see the coefficients of the model. We can check that these are what we had calculated previously. (Minus some rounding `R` is doing to display the results.)

```{r}
c(beta_0_hat, beta_1_hat)
```

Next, it would be nice to add the fitted line to the scatterplot. To do so we will use the `abline()` function.

```{r}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 3,
     col  = "dodgerblue")
abline(stop_dist_model, lwd = 3, col = "darkorange")
```

The `abline()` function is used to add lines of the form $a + bx$ to a plot. (Hence **`ab`**`line`.) When we give it `stop_dist_model` as an arguement, it automatically extracts the regression coefficients and uses them as the slope and intercept of the line. Here we also use `lwd` to modify the width of the line, as well as `col` to modify the color of the line.

```{r}
names(stop_dist_model)
```

```{r}
stop_dist_model$residuals
```

```{r}
coef(stop_dist_model)
resid(stop_dist_model)
fitted(stop_dist_model)
```

```{r}
summary(stop_dist_model)
names(summary(stop_dist_model))
```

```{r}
summary(stop_dist_model)$r.squared
summary(stop_dist_model)$coefficients
```

```{r}
summary(stop_dist_model)$sigma
```

TODO: Residual Standard Error, what `R` calls $s_e$, is stored in `summary(model)$sigma`.

\[
RSE = s_e = \sqrt{\frac{1}{n - 2}\sum_{i = 1}^n e_i^2}
\]

```{r}
sum(stop_dist_model$residuals ^ 2)
sum((cars$dist - fitted(stop_dist_model)) ^ 2)
sqrt(sum(stop_dist_model$residuals ^ 2) / (length(stop_dist_model$residuals) - 2))
```

```{r}
predict(stop_dist_model, data.frame(speed = 10))
```

```{r}
as.numeric(coef(stop_dist_model)[1] + coef(stop_dist_model)[2] * 10)
```

```{r}
predict(stop_dist_model, data.frame(speed = c(10, 20, 30, 40)))
```

```{r}
predict(stop_dist_model, data.frame(speed = cars$speed))
predict(stop_dist_model)
fitted(stop_dist_model)
```

## MLE Approach

Recall the model,

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $e_i \sim N(0, \sigma^2)$.

Then we can find the mean and variance of each $Y_i$.

\[
E[Y_i] = \beta_0 + \beta_1 x_i
\]

and

\[
Var[Y_i] = \sigma^2.
\]

TODO: define conditional distribution? $E[Y | X = x]$ ... Probably not.

Recall that the pdf of a random variable $X \sim N(\mu, \sigma^2)$ is given by

\[
f_{X}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left[-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2\right]}.
\]

TODO: define $f_{X}(x; \mu, \sigma^2)$ notation. Function of $x$, where $\mu$ and $\sigma^2$ are considered fixed and known.

Then we can write the pdf of each of the $Y_i$ is given as

\[
f_{Y_i}(y_i; x_i, \beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left[-\frac{1}{2}\left(\frac{y_i - (\beta_0 + \beta_1 x_i)}{\sigma}\right)^2\right]}.
\]

Given $n$ data points $(x_i, y_i)$ we can write the likelihood, which is a function of the three parameters $\beta_0$, $\beta_1$, and $\sigma^2$. (Since the data have been observed, we use lower case $y_i$ do note that these values are no longer random.)

\[
L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left[-\frac{1}{2}\left(\frac{y_i - \beta_0 - \beta_1 x_i}{\sigma}\right)^2\right]}
\]

Our goal is to find values of $\beta_0$, $\beta_1$, and $\sigma^2$ which maximize this function, which is a straightforward multivariate calculus problem.

We'll start by doing a bit of rearranging to make our task easier.

\[
L(\beta_0, \beta_1, \sigma^2) = \left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)^n \exp{\left[-\frac{1}{2 \sigma^2} \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2\right]}
\]

Then, as is often the case with finding MLEs, for mathematical convenience we will take the natural logarithm. of the likelihood function, then proceed to maximum the log-likelihood. (The resulting estimates will be the same since log is a monotonically increasing function.)

\[
\log L(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\log(2 \pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2 \sigma^2} \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
\]

Note that we use $\log$ to mean the natural logarithm. We now take a partial derivative with respect to each of the parameters.

\[
\begin{aligned}
\frac{\partial \log L(\beta_0, \beta_1, \sigma^2)}{\partial \beta_0} &= -\frac{2}{\sigma^2} \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)\\
\frac{\partial \log L(\beta_0, \beta_1, \sigma^2)}{\partial \beta_1} &= -\frac{2}{\sigma^2} \sum_{i = 1}^{n}(x_i)(y_i - \beta_0 - \beta_1 x_i) \\
\frac{\partial \log L(\beta_0, \beta_1, \sigma^2)}{\partial \sigma^2} &= -\frac{n}{2 \sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
\end{aligned}
\]

We then set each of the partial derivates equal to zero and solving the resulting system of equations.

\[
\begin{aligned}
\sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i) &= 0\\
\sum_{i = 1}^{n}(x_i)(y_i - \beta_0 - \beta_1 x_i) &= 0\\
-\frac{n}{2 \sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 &= 0
\end{aligned}
\]

You may notice that the first two equations also appears in the least squares approrach. Then, skipping the issue of actually checking if we have found a maximum, we then arrive at our estimates.

\[
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}}{\sum_{i = 1}^{n} x_i^2 - \frac{(\sum_{i = 1}^{n} x_i)^2}{n}} = \frac{S_{xy}}{S_{xx}}\\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x}\\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2
\end{aligned}
\]

Note that $\hat{\beta}_0$ and $\hat{\beta}_1$ are the same as the least squares estimates. However we now have a new estimate of $\sigma^2$, that is $\hat{\sigma}^2$. So we now have two different estimates of $\sigma^2$.

\[
\begin{aligned}
s_e^2 &= \frac{1}{n - 2} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 = \frac{1}{n - 2} \sum_{i = 1}^{n}e_i^2 & \text{Least Squares}\\
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i = 1}^{n}e_i^2 & \text{MLE}
\end{aligned}
\]

In the next chapter, we will discuss in detail the difference between these two estimates, which involves biasedness.

## Simulating SLR

TODO: again, simualtion is useful, and R makes it easy

TODO: this is an "ok" way to do the simulation:

```{r}
n       = 20
beta_0  = 5
beta_1  = 2
sigma   = 1
epsilon = rnorm(n, mean = 0, sd = sigma)
```

```{r}
x = runif(n, 0, 10)
y = beta_0 + beta_1 * x + epsilon
```

TODO: explain uniform
TOOD: explain relationship to model, especially WRT how eps is simulated.

```{r}
sim_fit = lm(y ~ x)
coef(sim_fit)
```

Note: cheating, being lazy, not making a data.frame. (easy in short term, bad idea in longterm)

```{r}
plot(y ~ x)
abline(sim_fit)
```

TODO: this is a better way to do the simulation

```{r}
sim_slr = function(n, beta_0 = 10, beta_1 = 5, sigma = 1, xmin = 0, xmax = 10) {
  epsilon = rnorm(n, mean = 0, sd = sigma)
  x       = runif(n, xmin, xmax)
  y       = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
```

```{r}
sim_data = sim_slr(n = 50, beta_0 = 5, beta_1 = 2, sigma = 3)
```

```{r}
head(sim_data)
```

```{r}
sim_fit = lm(response ~ predictor, data = sim_data)
coef(sim_fit)
```

```{r}
plot(response ~ predictor, data = sim_data,
     xlab = "Simulated Predictor Variable",
     ylab = "Simulated Response Variable",
     main = "Simulated Regression Data",
     pch  = 20,
     cex  = 2,
     col  = "dodgerblue")
abline(sim_fit, lwd = 3, col = "darkorange")
```

### Explanation versus Prediction

TODO: move this earlier for motivation
TODO: just show earlier in video for motivation?

Suppose you are a car manufacturer. You are an engineer tasked with something about braking. **Explanation.** Currently just linear.

- Causation
- Association (Correlation)

Suppose you own a car. How long does it take you to stop when you're driving a certain speed. **Prediction**

- Don't need to make the distinction.

One tool will do both: linear regression.

How can you use this data to:

- Explain relationship
    - Significant?
    - Which *variables* (word?) are most important? (Say, a variable for public/private)
- Predict

One tool will do both, LINEAR REGRESSION

EXPLAIN: You are a car manufacturer and want to know what factors influence stopping distance.
PREDICT: You own a car. If you drive a certain speed, how long will it take you to stop?

TODO: car insurance explaination (shifting perspectives, also do this for braking)
TODO: Note about CORR vs CAUSE: car insurance (if not done above.)

TODO: note about $(\bar{x}, \bar{y})$

TODO: note about $\sum_{i = 1}^{n} e_i = 0$. Other helpful expressions? Next time?

TODO: interpret $\hat{\beta}_0$ and $\hat{\beta}_1$, without hats? talk about with interpolating?

TODO: History, only if time.


