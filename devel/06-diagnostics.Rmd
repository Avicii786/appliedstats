# Model Diagnostics

> "Your assumptions are your windows on the world. Scrub them off every once in a while, or the light won't come in."
>
> --- **Isaac Asimov**

After reading this chapter you will be able to:

- Understand the assumptions of a regression model.
- Assess regression model assumptions using visualizations and tests.
- Understand leverage, outliers and influential points.
- Be able to identify unusual observations in regression models.

## Model Assumptions

Recall the multiple linear regression model that we have defined.

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)} + \epsilon_i, \qquad i = 1, 2, \ldots, n.
\]

Using matrix notation, this model can be written much more succinctly.

\[
Y = X \beta + \epsilon
\]

We found the estimates for the $\beta$ parameters using,

\[
\hat{\beta} = \left(  X^\top X  \right)^{-1}X^\top y.
\]

We than noted that these estimates had mean

\[
E[\hat{\beta}] = \beta,
\]

and variance

\[
Var[\hat{\beta}] = \sigma^2 \left(  X^\top X  \right)^{-1}.
\]

In particular, an individual parameter, say $\hat{\beta}_j$ had a normal distribution

\[
\hat{\beta}_j \sim N\left(\beta_j, \sigma^2 C_{jj}  \right)
\]

where $C$ was the matrix defined as

\[
C = \left(X^\top X\right)^{-1}.
\]

We then used this fact to define the following

\[
\frac{\hat{\beta}_j - \beta_j}{s_e \sqrt{C_{jj}}} \sim t_{n-p},
\]

which we used to perform hypothesis testing.

So far we have looked at various metrics such as RMSE, RSE and $R^2$ to determine how well our model fit our data. Each of these in some way considers the expression

\[
\sum (y_i - \hat{y}_i)^2.
\]

So, essentially each of these looks at how close the data points are to the model. However is that all we care about?

- It could be that the errors are made in a systematic way, which means that our model is misspecified. We may need additional interaction terms, or polynomial terms which we will see later.
- It is also possible that at a particular set of predictor values, the errors are very small, but at a different set of predictor values, the errors are large.
- Perhaps most of the errors are very small, but some are very large. This would suggest that the errors do not follow a normal distribution.

Are these issues that we care about? If all we would like to do is predict, possibly not, since we would only care about the size of our errors. However, if we would like to perform inference, for example to determine if a particular predictor is important, we care a great deal. All of the distributional results, such as a $t$-test for a single predictor, are derived under the assumptions of our model.

Technically, the assumptions of the model are encoded directly in a model statement such as,

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2).$

Often, the **assumptions of linear regression**, are stated as,

- **L**inearity: the response can be written as a linear combination of the predictors. (With noise about this true linear relationship.)
- **I**ndependence: the errors are independent.
- **N**ormality: the distribution of the errors should follow a normal distribution.
- **E**qual Variance: the error variance is the same at any set of predictor values.

The linearity assumption is encoded as

\[
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)},
\]

while the remaining three, are all encoded in

\[
\epsilon_i \sim N(0, \sigma^2),
\]

since the $\epsilon_i$ are $iid$ normal random variables with constant variance.

If these assumptions are met, great! We can perform inference, **and it is valid**. If these assumptions are *not* met, we can still perform a $t$-test using `R`, but the results are **not valid**. The distributions of the parameter estimates will not be what we expect. Hypothesis tests will then accept or reject incorrectly. Essentially, **garbage in, garbage out.**

## Checking Assumptions

We'll now look at a number of tools for checking the assumptions of a linear model.

### Fitted versus Residuals Plot

Probably our most useful tool will be a **Fitted versus Residuals Plot**. It will be useful for checking both the **linearity** and **constant variance** assumptions.

First, let's consider a true SLR model,

\[
Y_i = 3 + 5x_i + \epsilon_i \quad \quad \epsilon_i \sim N(0, 1)
\]

This model does not violate any the assumptions, so we'll use this to see what a good fitted versus residuals plot should look like. First, we'll simulate observations from this model.

```{r}
n = 500
set.seed(42)
sim_data = data.frame(x = runif(n) * 5, y = rep(0, n))
sim_data$y = 3 + 5 * sim_data$x + rnorm(n, 0, 1)
head(sim_data)
```

We then fit the model and add the fitted line to a scatterplot.

```{r}
plot(y ~ x, data = sim_data, col = "dodgerblue")
fit1 = lm(y ~ x, data = sim_data)
abline(fit1, col = "darkorange", lwd = 3)
```

We now plot a fitted versus residuals plot. Note, this is residuals on the $y$-axis despite the ordering in the name. Sometimes you will see this called a residuals versus fitted, or residuals versus predicted plot.

```{r}
plot(fitted(fit1), resid(fit1), col = "dodgerblue", xlab = "Fitted", ylab = "Residual")
abline(h = 0, col = "darkorange", lwd = 2)
```

We should look for two things in this plot. 

- At any fitted value, the mean of the residuals should be roughly 0. If this is the case, the *linearity* assumption is valid. For this reason, we generally add a horizontal line at $y = 0$ to emphasize this point.
- At every fitted value, the spread of the residuals should be roughly the same. If this is the case, the *constant variance* assumption is valid.

Here we see this is the case for both.

To get a better idea of how a fitted versus residuals plot can be useful, we will simulate from models with violated assumptions.

We'll first demonstrate a model with non-constant variance. In this case, the variance is larger for larger values of the predictor variable $x$.

```{r}
sim_data2 = sim_data
sim_data2$y = 3 + 5 * sim_data2$x + rnorm(n, 0, sim_data2$x)
fit2 = lm(y ~ x, data = sim_data2)
plot(y ~ x, data = sim_data2, col = "dodgerblue")
abline(fit2, col = "darkorange", lwd = 3)
```

This actually is rather easy to see by added the fitting line to a scatterplot in this case. This is because we are only performing simple linear regression. With multiple regression, a fitted versus residuals plot is a necessity, since adding a fitted regression to a scatterplot isn't exactly possible.

```{r}
plot(fitted(fit2), resid(fit2), col = "dodgerblue", xlab = "Fitted", ylab = "Residual")
abline(h = 0, col = "darkorange", lwd = 2)
```

On the fitted versus residuals plot, we see two things very clearly. For any fitted values, the residuals seem roughly centered at 0. This is good! The linearity assumption is not violated. However, we also see very clearly, that for larger fitted values, the spread of the residuals is larger. This is bad! The constant variance assumption is violated here.

Now we will demonstrate a model which does not meet the linearity assumption.

```{r}
sim_data3 = sim_data
sim_data3$y = 3 + 5 * sim_data3$x ^ 2 + rnorm(n, 0, 5)
fit3 = lm(y ~ x, data = sim_data3)
plot(y ~ x, data = sim_data3, col = "dodgerblue")
abline(fit3, col = "darkorange", lwd = 3)
```

Again, this is rather clear on the scatterplot, but again, we wouldn't be able to check this plot for multiple regression.

```{r}
plot(fitted(fit3), resid(fit3), col = "dodgerblue", xlab = "Fitted", ylab = "Residual")
abline(h = 0, col = "darkorange", lwd = 2)
```

This time on the fitted versus residuals plot, for any fitted value, the spread of the residuals is about the same. However, they are not even close to centered at zero! So the constant variance assumption is met, but the linearity assumption is violated. Our model is simply wrong. (We're trying to fit a line to a curve!)

### Breusch-Pagan Test

Constant variance is often called **homoscedasticity**. Conversely, non-constant variance is called **heteroscedasticity**. We've seen how we can use a fitted versus residuals plot to look for these attributes.

While a fitted versus residuals plot can give us an idea about homoscedasticity, sometimes we would prefer a more formal test. There are many test for constant variance, but here we will present one, the [**Breusch-Pagan Test**](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test). The exact details of the test will omitted here, but importantly the null and alternative can be considered to be,

- $H_0$: Homoscedasticity. The errors have constant variance about the true model.
- $H_1$: Heteroscedasticity.  The errors have non-constant variance about the true model.

Isn't that convenient? A test that will specifically test the **constant variance** assumption.

The Breusch-Pagan Test can not be performed by default in `R`, however the function `bptest` in the `lmtest` package implements the test.

```{r, message = FALSE, warning = FALSE}
#install.packages("lmtest")
library(lmtest)
```

Let's try it on the three model we fit above. Recall,

- `fit1` had no violation of assumptions,
- `fit2` violated the constant variance assumption, but not linearity,
- `fit3` violated linearity, but not constant variance.

```{r}
bptest(fit1)
```

For `fit1` we see a large p-value, so we do not reject the null of homoscedasticity, which is what we would expect.

```{r}
bptest(fit2)
```

For `fit2` we see a small p-value, so we reject the null of homoscedasticity. The constant variance assumption is violated. This matches our findings with a fitted versus residuals plot.

```{r}
bptest(fit3)
```

Lastly, for `fit3` we again see a large p-value, so we do not reject the null of homoscedasticity, which matches our findings with a fitted versus residuals plot.

### Histogram

We have a number of tools for assessing the normality assumption. The most obvious would be to make a histogram of the residuals. If it appears roughly normal, then we'll believe the errors could truly be normal.

```{r}
par(mfrow = c(1, 3))
hist(resid(fit1),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit1",
     col    = "darkorange",
     border = "dodgerblue")
hist(resid(fit2),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit2",
     col    = "darkorange",
     border = "dodgerblue")
hist(resid(fit3),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit3",
     col    = "darkorange",
     border = "dodgerblue")
```

Above are histograms for each of the three regression we have been considering. Notice that the first, for `fit1` appears very normal. The third, for `fit3`, appears to be very non-normal. However `fit2` is not as clear. It does have a rough bell shape, however, it also has a very sharp peak. For this reason we will usually use more powerful tools such as **Q-Q plots** and the **Shapiro-Wilk test** for assessing the normality of errors.

### Q-Q Plots

Another visual method for assessing the normality of errors, which is more powerful than a histogram, is a normal quantile-quantile Plot, or **Q-Q plot** for short.

In `R` these are very easy to make. The `qqnorm()` function plots the points, and the `qqline()` function adds the necessary line. We create a Q-Q plot for the residuals of `fit1` to check if the errors could truly be normal.

```{r}
qqnorm(resid(fit1), main = "Normal Q-Q Plot, fit1", col = "dodgerblue")
qqline(resid(fit1), col = "darkorange", lwd = 2)
```

In short, if the points of the plot do not closely follow a straight line, this would suggest that the data do not come from a normal distribution.

The calculations required to create the plot vary depending on the implementation, but essentially the $y$-axis is the sorted data (observed, or sample quantiles), and the $x$-axis is the values we would expect if the data did come from a normal distribution (theoretical quantiles).

The Wikipedia page for Normal probability plots gives details on how this is implemented in `R` if you are interested. [Wikipedia: Normal probability plot](http://en.wikipedia.org/wiki/Normal_probability_plot)

Also, to get a better idea of how Q-Q plots work, here is a quick function which creates a Q-Q plot:

```{r}
qq_plot = function(w) {

  n = length(w)
  normal_quantiles = qnorm(((1:n) / (n + 1)))

  # plot theoretical verus observed quantiles
  plot(normal_quantiles, sort(w),
       xlab = c("Theoretical Quantiles"),
       ylab = c("Sample Quantiles"),
       col = "dodgerblue")
  title("Normal Q-Q Plot")

  ## calculate line through the first and third quartiles  
  slope     = (quantile(w, 0.75) - quantile(w, 0.25)) / (qnorm(0.75) - qnorm(0.25))
  intercept = quantile(w, 0.25) - slope * qnorm(0.25)

  # add to existing plot
  abline(intercept, slope, lty = 2, lwd = 2, col = "darkorange")
}
```

We can then verify that it is essentially equivalent to using `qqnorm()` and `qqline()` in `R`. There are *slight* differences, but the general idea is the same.

```{r}
set.seed(420)
x = rnorm(100, mean = 0 , sd = 1)
par(mfrow = c(1, 2))
qqnorm(x, col = "dodgerblue")
qqline(x, lty = 2, lwd = 2, col = "darkorange")
qq_plot(x)
```

To get a better idea of what "close to the line" means, we perform a number of simulations, and create Q-Q plots.

First we simulate data from a normal distribution with different sample sizes, and each time create a Q-Q plot.

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rnorm(10))
qq_plot(rnorm(25))
qq_plot(rnorm(100))
```

Since this data **is** sampled from a normal distribution, these are all, by definition, good Q-Q plots. The points are "close to the line" and we would conclude that this data could have been sampled from a normal distribution. Notice in the first plot, one point is *somewhat* far from the line, but just one point, in combination with the small sample size, is not enough to make use worried. We see with the large sample size, all of the points are rather close to the line.

Next, we simulate data from a $t$ distribution with a small degrees of freedom, for different sample sizes.

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rt(10, df = 4))
qq_plot(rt(25, df = 4))
qq_plot(rt(100, df = 4))
```

Recall, that as the degrees of freedom for a $t$ distribution become larger, the distribution becomes more and more similar to a normal. Here, using 4 degrees of freedom, we have a distribution that is somewhat normal, it symmetrical and roughly bell-shaped, however it has "fat tails." This presents itself clearly in the third panel. While many of the points are close to the line, at the edges, there are large discrepancies. This indicates that the values or to small (negative) or too large (positive) compared to what we would expect for a normal distribution. Some for the sample size of `100`, we would conclude that that normality assumption is violated. (If these were residuals of a model.) For sample sizes of `10` and `25` we may be suspicious, but not entirely confident. Reading Q-Q plots, is a bit of an art, not completely a science.

Next, we simulate data from an exponential distribution.

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rexp(10))
qq_plot(rexp(25))
qq_plot(rexp(100))
```

This is a distribution that is not very similar to a normal, so in all three cases, we see points that are far from the lines, so we would think that the normality assumption is violated.

For a better understanding of which Q-Q plots are "good," repeat the simulations above a number of times (without setting the seed) and pay attention to the differences between those that are simulated from normal, and those that are not. Also consider different samples sizes and distribution parameters.

Returning to our three regression, recall,

- `fit1` had no violation of assumptions,
- `fit2` violated the constant variance assumption, but not linearity,
- `fit3` violated linearity, but not constant variance.

We'll now create a Q-Q plot for each to asses normality of errors.

```{r}
qqnorm(resid(fit1), main = "Normal Q-Q Plot, fit1", col = "dodgerblue")
qqline(resid(fit1), col = "darkorange", lwd = 2)
```

For `fit1`, we have a near perfect Q-Q plot. We would believe the errors follow a normal distribution.

```{r}
qqnorm(resid(fit2), main = "Normal Q-Q Plot, fit2", col = "dodgerblue")
qqline(resid(fit2), col = "darkorange", lwd = 2)
```

For `fit2`, we have a suspect Q-Q plot. We would probably **not** believe the errors follow a normal distribution.

```{r}
qqnorm(resid(fit3), main = "Normal Q-Q Plot, fit3", col = "dodgerblue")
qqline(resid(fit3), col = "darkorange", lwd = 2)
```

Lastly, for `fit3`, we again have a suspect Q-Q plot. We would probably **not** believe the errors follow a normal distribution.

### Shapiro-Wilk Test

Histograms and Q-Q Plots give a nice visual representation of the residuals distribution, however if we are interested in formal testing, there are a number of options available. A commonly used test is the **Shapiro–Wilk test**, which is implemented in `R`.

```{r}
set.seed(42)
shapiro.test(rnorm(25))
shapiro.test(rexp(25))
```


This gives us the value of the test statistic and its p-value. The null hypothesis assumes the data follow a normal distribution, thus a small p-value indicates we believe there is only a small probability the data follow a normal distribution.

For details, see: 
[Wikipedia: Shapiro–Wilk test](https://en.wikipedia.org/wiki/Shapiro-Wilk_test)

In the above examples, we see we fail to reject for the data samples from normal, and reject on the non-normal data.

Returning again to `fit1`, `fit2` and `fit3`, we see the result of running `shapiro.test()` on the residuals of each, returns a result for each that matches for decisions based on the Q-Q plots.

```{r}
shapiro.test(resid(fit1))
```

```{r}
shapiro.test(resid(fit2))
```

```{r}
shapiro.test(resid(fit3))
```

## Unusual Observations

In addition to checking the assumptions of regression, we often also look for an "unusual observations" in the data. Sometimes certain data points can have an extremely large influence on a regression, so much so that...

The following three plots are inspired by an example from [Linear Models with R](http://www.maths.bath.ac.uk/~jjf23/LMR/).

```{r}
par(mfrow = c(1, 3))
set.seed(42)
ex_data  = data.frame(x = 1:10, y = 10:1 + rnorm(10))
ex_model = lm(y ~ x, data = ex_data)

# low leverage, big outlier, small influence
point1 = c(5.4, 11)
model1 = lm(y ~ x, data = rbind(ex_data, point1))
plot(y ~ x, data = rbind(ex_data, point1), cex = 1.5)
points(x = point1[1], y = point1[2], pch = 4, cex = 5, col = "firebrick", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model1, lty = 2, col = "darkorange", lwd = 2)

# high leverage, small outlier, low influence
point2 = c(15, -4.1)
model2 = lm(y ~ x, data = rbind(ex_data, point2))
plot(y ~ x, data = rbind(ex_data, point2), cex = 1.5)
points(x = point2[1], y = point2[2], pch = 4, cex = 5, col = "firebrick", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model2, lty = 2, col = "darkorange", lwd = 2)

# high leverage, big outlier, large influence
point3 = c(15, 5.1)
model3 = lm(y ~ x, data = rbind(ex_data, point3))
plot(y ~ x, data = rbind(ex_data, point3), cex = 1.5)
points(x = point3[1], y = point3[2], pch = 4, cex = 5, col = "firebrick", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model3, lty = 2, col = "darkorange", lwd = 2)
```



```{r}
coef(ex_model)[2]
coef(model1)[2]
coef(model2)[2]
coef(model3)[2]
```







### Leverage

Recall that,

\[
\hat{\beta} = \left(X^\top X \right)^{-1} X^\top y.
\]

Thus,

\[
\hat{y}= X \hat{\beta}   = X \left(X^\top X \right)^{-1} X^\top y
\]

Now we define,

\[
H = X \left(X^\top X\right)^{-1} X^\top
\]

Which we will refer to as the hat matrix. The hat matrix is used to project onto the subspace spanned by the columns of $\boldsymbol{X}$. (And is otherwise know as a projection matrix.)

The diagonal elements of this matrix are called the leverages.

\[
H_{ii} = h_i
\]

Large values of $h_i$ indicate extreme values in $X$, which may influence regression. Note that leverages only depend on $X$.

\[
\sum h_i = p
\]

Here, $p$ is the number of $\beta$s. (Also the trace (and rank) of the hat matrix.)

A common check for a large leverage is to compare to $2*\frac{p}{n} = 2\bar{h}$, two times the average leverage. A leverage larger than this is considered an observation to be aware of.

For simple linear regression, we have,

\[
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{S_{xx}}
\]

which suggests that the large leverages occur when $x$ values are far from their mean. (Recall that the regression goes through the point $(\bar{x},\bar{y})$.)

There are multiple ways to find leverages in `R`.

```{r}
lev_ex = data.frame(
  x1 = c(0, 11, 11, 7, 4, 10, 5, 8),
  x2 = c(1, 5, 4, 3, 1, 4, 4, 2),
  y  = c(11, 15, 13, 14, 0, 19, 16, 8))

plot(x2 ~ x1, data = lev_ex, cex = 2)
points(7, 3, pch = 20, col = "red", cex = 2)

X = cbind(rep(1, 8), lev_ex$x1, lev_ex$x2)
H = X %*% solve(t(X) %*% X) %*% t(X)
diag(H)
sum(diag(H))

lev_fit = lm(y ~ ., data = lev_ex)
coef(lev_fit)
hatvalues(lev_fit)
```

```{r}
which.max(hatvalues(lev_fit))

lev_ex_1 = lev_ex
lev_ex_1$y[1] = 20

lm(y ~ ., data = lev_ex_1)

# note, leverages wouldn't change, we haven't changed X

which.min(hatvalues(lev_fit))

lev_ex_2 = lev_ex
lev_ex_2$y[4] = 30

lm(y ~ ., data = lev_ex_2)

mean(lev_ex$x1)
mean(lev_ex$x2)
lev_ex[4,]
```





```{r}
hatvalues(model1)
hatvalues(model2)
hatvalues(model3)
hatvalues(model1) > 2 * mean(hatvalues(model1))
hatvalues(model2) > 2 * mean(hatvalues(model2))
hatvalues(model3) > 2 * mean(hatvalues(model3))
```

### Outliers

Outliers are points which do not fit the model well. They may or may not have a large affect on the model. To identify outliers, we will look for observations with large residuals.

Note,

\[
\boldsymbol{e} = \boldsymbol{Y} - \boldsymbol{\hat{Y}} = (\boldsymbol{I}-\boldsymbol{H})\boldsymbol{Y}
\]

Then, under the assumptions of linear regression,

\[
Var(e_i) = (1-h_i) \sigma^2
\]

and thus estimating $\sigma^2$ with $s_e^2$ gives

\[
SE[e_i] = s_e \sqrt{(1 - h_i)}.
\]

We can then look at the **standardized residual** for each observation, $i = 1, 2, \ldots n$,

\[
r_i = \frac{e_i}{s_e\sqrt{1 - h_i}} \overset{approx}{\sim} N(\mu = 0, \sigma^ 2 = 1)
\]

when $n$ is large.

```{r}
resid(model1)
rstandard(model1)
rstandard(model1)[abs(rstandard(model1)) > 2]
```

```{r}
resid(model2)
rstandard(model2)
rstandard(model2)[abs(rstandard(model2)) > 2]
```

```{r}
resid(model3)
rstandard(model3)
rstandard(model3)[abs(rstandard(model3)) > 2]
```

### Influence

As we have seen in the plots from the book, some outliers only change the regression a small amount (plot 1) and some outliers have a large effect on the regression. (plot 3) Observations that fall into the later category we will call **influential**.

A common measure of influence is **Cook's Distance**,

\[
  D_i = \frac{1}{p}r_i^2\frac{h_i}{1-{h_i}}
\]

which is a function of both leverage and standardized residuals. A Cook's Distance is considered large if $D_i > 4/n$, and an observation with a large Cook's Distance is called influential.

```{r}
cooks.distance(model1) > 4 / length(cooks.distance(model1))
cooks.distance(model2) > 4 / length(cooks.distance(model2))
cooks.distance(model3) > 4 / length(cooks.distance(model3))
```

## Maybe a complete example, or two

### Good

```{r}
mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)

plot(fitted(mpg_hp_add), resid(mpg_hp_add), col = "dodgerblue", xlab = "Fitted", ylab = "Residual")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(mpg_hp_add), col = "dodgerblue")
qqline(resid(mpg_hp_add), col = "darkorange", lwd = 2)

bptest(mpg_hp_add)
shapiro.test(resid(mpg_hp_add))

sum(cooks.distance(mpg_hp_add) > 4 / length(cooks.distance(mpg_hp_add)))
```

```{r, fig.height = 8, fig.width = 8}
par(mfrow = c(2, 2))
plot(mpg_hp_add)
```

### Suspect

```{r, echo = FALSE}
# read data frame from the web
autompg = read.table(
  "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data",
  quote = "\"",
  comment.char = "",
  stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin", "name")
# remove missing data, which is stored as "?"
autompg = subset(autompg, autompg$hp != "?")
# remove the plymouth reliant, as it causes some issues
autompg = subset(autompg, autompg$name != "plymouth reliant")
# give the dataset row names, based on the engine, year and name
rownames(autompg) = paste(autompg$cyl, "cylinder", autompg$year, autompg$name)
# remove the variable for name
autompg = subset(autompg, select = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin"))
# change horsepower from character to numeric
autompg$hp = as.numeric(autompg$hp)
# create a dummary variable for foreign vs domestic cars. domestic = 1.
autompg$domestic = as.numeric(autompg$origin == 1)
# remove 3 and 5 cylinder cars (which are very rare.)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]
# the following line would verify the remaining cylinder possibilities are 4, 6, 8
#unique(autompg$cyl)
# change cyl to a factor variable
autompg$cyl = as.factor(autompg$cyl)
```

```{r}
str(autompg)
```

```{r}
big_model = lm(mpg ~ disp * hp * domestic, data = autompg)
qqnorm(resid(big_model), col = "dodgerblue")
qqline(resid(big_model), col = "darkorange", lwd = 2)
shapiro.test(resid(big_model))

big_mod_cd = cooks.distance(big_model)

big_model_fix = lm(mpg ~ disp * hp * domestic, data = autompg, subset = big_mod_cd < 4 / length(big_mod_cd))
qqnorm(resid(big_model_fix), col = "dodgerblue")
qqline(resid(big_model_fix), col = "darkorange", lwd = 2)
shapiro.test(resid(big_model_fix))
```

- "robust"

- WHAT DO WE DO NOW!

- here we modified the data to make the model better, next chapter we'll modify the model to make the model better.
