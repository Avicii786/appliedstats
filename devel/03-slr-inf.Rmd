# Inference for Simple Linear Regression

> "There are three types of lies: lies, damn lies, and statistics."
>
> --- **Benjamin Disraeli**

After reading this chapter you will be able to:

- ...
- ...
- ...


## Sampling Distributions

"notation abuse"

- $\hat{\beta_0}$ is a **statistic** with data, random variable without
- $\hat{\beta_1}$ is a **statistic** with data, random variable without

- last time we got estimates
    - what we have (point) estimated, how we have estimated it
- are they any good?
- state gauss markov
- show beta_1 is lc of Y values
- unbiased and min var good
    - save for mathstat class to show
    - we will use simualtion
- distributional results
- show with simulation
     - see below

```{r}
# note, this is not the "best" way to do this, but shows it well
# note, need to "fix" x
set.seed(42)
num_samples = 1000 # make this larger
sample_size = 20 # this is n
beta_0      = 10
beta_1      = 2
sigma       = 3
x = runif(sample_size, 0, 10)
Sxx = sum((x - mean(x)) ^ 2)

beta_0_hat = rep(0, num_samples)
beta_1_hat = rep(0, num_samples)

for(i in 1:num_samples) {
  y = beta_0 + beta_1 * x + rnorm(sample_size, mean = 0, sd = sigma)
  sim_model = lm(y ~ x)
  
  beta_0_hat[i] = coef(sim_model)[1]
  beta_1_hat[i] = coef(sim_model)[2]
}



mean(beta_1_hat)
beta_1
sd(beta_1_hat)
sigma / sqrt(Sxx)
hist(beta_1_hat, prob = TRUE, breaks = 25, xlab = expression(hat(beta)[1]), main = "")
ex_beta_1 = beta_1
sd_beta_1 = sigma / sqrt(Sxx)
curve(dnorm(x, mean = ex_beta_1, sd = sd_beta_1), col = 2, add = TRUE)

hist(beta_0_hat)
mean(beta_0_hat)
beta_0
sd(beta_0_hat)
sigma * sqrt(1 / sample_size + mean(x) ^ 2 / Sxx)

```


## Confidence Intervals

- SD vs SE
- (EST - TRUE) / SD = NORMAL
- (EST - TRUE) / SE = t
- hand waive derivation of test stat dist for both reg coefs
    - Z / sqrt(X^2 / d)
- for each of the estimates
    - no CI for sigma

## Hypothesis Test

- setup testing as (EST - HYP) / SE, define SE in all case?
- [https://xkcd.com/892/](https://xkcd.com/892/)

## CIs and PIs

- note about $(\bar{x}, \bar{y})$
    - also, this is where bands are smallest

## F Test

recap R^2





TODO: note about $\sum_{i = 1}^{n} e_i = 0$. Other helpful expressions?



\[
Y_i \sim N(\beta_0 + \beta_1, \sigma^2)
\]

\[
S_{xy} = \sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}  = \sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})
= \sum_{i = 1}^{n}(x_i - \bar{x})(y_i)
\]

\[
\hat{\beta_1} = \frac{S_{xy}}{S_xx} = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(Y_i)}{\sum_{i = 1}^{n}(x_i - \bar{x})^2} \sim N\left(  \beta_1, \frac{\sigma^2}{\sum_{i = 1}^{n}(x_i - \bar{x})^2} \right)
\]

More succinctly...

\[
\hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{x} \sim N\left(  \beta_0, \frac{\sigma^2 \sum_{i = 1}^{n}x_i^2}{n \sum_{i = 1}^{n}(x_i - \bar{x})^2} \right)
\]

More often, more succinctly....


TODO: note notation abuse, switching between $Y_i$ and $y_i$ at liberty.

!TODO: derivations, unbiasedness
!TODO: decide how much to show?

\[
Var[\hat{\beta_1}] = \frac{\sigma^2}{S_{xx}}
\]

\[
SD[\hat{\beta_1}] = \frac{\sigma}{\sqrt{S_{xx}}}
\]

\[
SE[\hat{\beta_1}] = \frac{s_e}{\sqrt{S_{xx}}}
\]

\[
t = \frac{\hat{\beta_1}-\beta_{10}}{SE[\hat{\beta_1}]} = \frac{\hat{\beta_1}-\beta_{10}}{s_e / \sqrt{S_{xx}}}
\]

\[
\hat{\beta_1} \pm t_{n - 2}(\alpha/2) SE[\hat{\beta_1}] 
\]


???

\[
\hat{\beta_1} \pm t_{n - 2, \alpha/2} \cdot SE[\hat{\beta_1}] 
\]

Possible winner:
(may need to edit previous)

\[
\hat{\beta_1} \pm t_{\alpha/2, n - 2} \cdot SE[\hat{\beta_1}] 
\]

\[
\hat{\beta_1} \pm t_{n - 2}^{\alpha/2} \cdot SE[\hat{\beta_1}] 
\]

\[
\hat{\beta_1} \pm t_{n - 2, \frac{\alpha}{2}} SE[\hat{\beta_1}] 
\]

\[
\hat{\beta_1} \pm t_{n - 2}(\alpha/2) \frac{s_e}{\sqrt{S_{xx}}}
\]



\[
t = \frac{\hat{\beta_0}-\beta_{00}}{s_e\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}}}
\]

\[
\hat{\beta_0} \pm t_{n - 2}(\alpha/2) s_e\sqrt{\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}}
\]

CI:

\[
\hat{y} \pm t_{n - 2}(\alpha/2)  s_e\sqrt{\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}
\]

PI:

\[
\hat{y} \pm t_{n - 2}(\alpha/2) \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}
\]



Y does **not* depend on x... (This, we will assume.)

\[
H_0: \beta_1 = 0 \quad \quad Y_i = \beta_0 + \epsilon_i
\]

Y does depend on x...

\[
H_1: \beta_1 \neq 0 \quad \quad Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Use the phrase "due to chance" (There is a picture to make here.)

VERY small p-value with `cars`, will see this in practice.

Then test a specific,

\[
H_0: \beta_1 = 10
\]


Be explicit about "nested models" with F test. Comparing the two models above. Projection picture.


\[
F = 
\]


stat significance vs practical significance, effect size

significance of LINEAR relationship. show line failing for quadratic plot.

CI is EST pm CRIT * SE
TEST is (EST - HYP) / SE

check if HYP is in CI = test

SE SE SE SE SE

no CI for sigma

all else equal, use the simpler model. (~1 is nested hierarchically inside ~ x)


"We may speak of this hypothesis as the 'null hypothesis', and it should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation." - RA Fisher