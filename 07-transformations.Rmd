# Transformations

> "Give me a lever long enough and a fulcrum on which to place it, and I shall move the world."
>
> --- **Archimedes**

After reading this chapter you will be able to:

- Understand the concept of a variance stabilizing transformation.
- Use transformations of the response to improve regression models.
- Use polynomial terms as predictors to fit more flexible regression models.

Last chapter we checked the assumptions of regression models and look at ways to diagnose possible issues. This chapter we will use transformations of both response and predictor variables in order to correct issues with model diagnostics, and to also potentially simply make a model fit better.

## Response Transformation

Let's look at some (fictional) salary data from the (fictional) company *Initech*. We will try to model `salary` as a function of `years` of experience. The data can be found in [`initech.csv`](`data/initech.csv`).

```{r}
initech = read.csv("../data/initech.csv")
```

We first fit a simple linear model.

```{r}
initech_fit = lm(salary ~ years, data = initech)
summary(initech_fit)
```

This model appears significant, but does it meet the model assumptions?

```{r}
plot(salary ~ years, data = initech, col = "dodgerblue", pch = 20, cex = 1.5)
abline(initech_fit, col = "darkorange", lwd = 2)
```

Adding the fitted line to the plot, we see that the linear relationship appears correct.

```{r}
plot(fitted(initech_fit), resid(initech_fit), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "orange", lwd = 2)
```

However, from the fitted versus residuals plot it appears there is non-constant variance. Specifically, the variance increases as the fitted value increases.

### Variance Stabilizing Transformations

Recall the fitted value is our estimate of the mean at a particular value of $x$. Under our usual assumptions,

\[
  \epsilon_i \sim N(0,\sigma^2)
\]

and thus

\[
  Var[Y | X = x] = \sigma^2
\]

which is a constant value for any value of $x$.

However, here we see that the variance is a function of the mean,

\[
  Var[Y | X = x] = h(\mu).
\]

In this case, $h$ is some increasing function.

In order to correct for this, we would like to find some function of $Y$, $g(Y)$ such that,

\[
  Var[g(Y) | X = x] = c
\]

where $c$ is a constant that does not depend on $\mu$. A transformation that accomplishes this is called a **variance stabilizaing transformation.**

A common variance stabilizing transformation (VST) when we see increasing variance in a fitted versus residuals plot is $\log(Y)$. Also, if the values of a variable range over more than one order of magnitude and the variable is *strictly positive*, then replacing the variable by its logarithm is likely to be helpful.

A reminder, that for our purposes, $\log$ and $\ln$ are both the natural log. `R` uses `log` to mean the natural log, unless a different base is specified.

We will now use a log transformed response for the *Initech* data,

\[
  \log(y_i) = \beta_0 + \beta_1 x_i + \epsilon_i.
\]

Note, if we re-scale the data from a log scale back to the original scale of the data, we now have

\[
  y_i = \exp(\beta_0 + \beta_1 x_i) \cdot \exp(\epsilon_i)
\]

which has the errors entering the model in a multiplicative fashion.

Fitting this model in `R` requires only a minor modification to our formula specification.

```{r}
initech_fit_log = lm(log(salary) ~ years, data = initech)
```

Note that we do not create a new variable, but simply transform the variable inside the model formula.

```{r}
plot(log(salary) ~ years, data = initech, col = "dodgerblue", pch = 20, cex = 1.5)
abline(initech_fit_log, col = "darkorange", lwd = 2)
```

Plotting the data on the transformed log scale and adding the fitted line, the relationship again appears linear, and we can already see that the variation about the fitted line looks constant.

- plot on log scale, appears linear
- plot on data scale, "non-linear", still linear

```{r}
plot(salary ~ years, data = initech, col = "dodgerblue", pch = 20, cex = 1.5)
curve(exp(initech_fit_log$coef[1] + initech_fit_log$coef[2] * x),
      from = 0, to = 30, add = TRUE, col = "darkorange", lwd = 2)
```

By plotting the data on the original scale, and adding the fitted regression, we see an exponential relationship. However, this is still a *linear* model, since the transformed response, $\log(y)$, is a *linear* combination of the predictors.

```{r}
plot(fitted(initech_fit_log), resid(initech_fit_log), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

The fitted versus residuals plot looks much better. It appears the constant variance assumption is no longer violated.

Comparing RMSE, we also see that the log transformed model simply fits better, with a smaller average squared error.

```{r}
sqrt(mean(resid(initech_fit) ^ 2))
sqrt(mean(resid(initech_fit_log) ^ 2))
```

But wait, that isn't fair, this difference is simply due to the different scales being used.

```{r}
sqrt(mean((initech$salary - fitted(initech_fit)) ^ 2))
sqrt(mean((initech$salary - exp(fitted(initech_fit_log))) ^ 2))
```

Transforming the fitted values of the log model back to the data scale, we do indeed see that it fits better!

```{r}
summary(initech_fit_log)
```

\[
  \log(\hat{y}) = \hat{\beta_0} + \hat{\beta_1}x  = 9.84 + 0.05x 
\]

Note, if we re-scale the data from a log scale back to the original scale of the data, we now have

\[
  \hat{y} = \exp(\hat{\beta_0})\exp(\hat{\beta_1}x) = \exp(9.84)\exp(0.05x)
\]

We see that for every one additional year of experience, average salary increases $\exp(0.05) = 1.051$ times. (We're multiplying, not adding.)

While using a $\log$ transform is possibly the most common response variable transformation many others exist. We'll now consider a family of transformations and choose the best from among them, which includes the $\log$ transform

### Box-Cox Transformations

The Box-Cox method considers a family of transformations on strictly positive response variables,

\[
g_\lambda(y) = \left\{
\begin{array}{lr}\displaystyle\frac{y^\lambda - 1}{\lambda} &  \lambda \neq 0\\
        & \\
       \log(y) &  \lambda = 0
     \end{array}
   \right.
\]

The $\lambda$ parameter is chosen by numerically by maximizing the log-likelihood,

\[
  L(\lambda) = -\frac{n}{2}\log(RSS_\lambda / n) + (\lambda -1)\sum \log(y_i).
\]

A $100(1 - \alpha)\%$ confidence interval for $\lambda$ is,

\[
    \left\{ \lambda :  L(\lambda) > L(\hat{\lambda}) - \frac{1}{2}\chi_{1,\alpha}^2  \right\}   
\]

which `R` will plot for us to help quickly select an appropriate $\lambda$. We often choose a "nice" value from within the confidence interval, instead of the value of $\lambda$ that truly maximizes the likelihood.

```{r}
library(MASS)
library(faraway)
```

Here we need the `MASS` package for the `boxcox()` function, and we will consider a couple of datasets from the `faraway` package.

First we'll use the `savings` dataset as an example of using the Box-Cox method to justify the use of no transformation. We fit an additive multiple regression model with `sr` as the response and each of the other variable as predictors.

```{r}
savings_model = lm(sr ~ ., data = savings)
```

We then use the `boxcox()` function to find the best transformation of the form consider by the Box-Cox method.

```{r}
boxcox(savings_model, plotit = TRUE)
```

`R` automatically plots the log-Likelihood as a function of possible $\lambda$ values.

```{r}
boxcox(savings_model, plotit = TRUE, lambda = seq(0.5, 1.5, by = 0.1))
```

Note that we can specify a range of $\lambda$ values to consider. We often specify a range that is more interesting. Here we see that $\lambda = 1$ is both in the interval, and extremely close to the maximum, which suggests a transformation of the form

\[
\frac{y^\lambda - 1}{\lambda} = \frac{y^1 - 1}{1} = y - 1.
\]

This is essentially not a transformation. It would not change the variance nor make the model fit better. By shifting every value by 1, we would only change the intercept of the model, and the resulting errors would be the same.

```{r}
plot(fitted(savings_model), resid(savings_model), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

Looking at a fitted versus residuals plot verifies that there likely are not any issue with the assumptions of this model, which Breusch-Pagan and Shapiro-Wilk tests verify.

```{r, message = FALSE, warning = FALSE}
library(lmtest)
bptest(savings_model)
shapiro.test(resid(savings_model))
```

Now we'll use the `savings` dataset as an example of using the Box-Cox method to justify a transformation other than $\log$. We fit an additive multiple regression model with `Species` as the response and most of the other variables as predictors.

```{r}
gala_model = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)
```

```{r}
plot(fitted(gala_model), resid(gala_model), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

Even though there is not a lot of data for large fitted values, it still seems very clear that the constant variance assumption is violated.

```{r}
boxcox(gala_model, lambda = seq(-0.25, 0.75, by = 0.05), plotit = TRUE)
```

Using the Box-Cox method, we see that $\lambda = 0.3$ is both in the interval, and extremely close to the maximum, which suggests a transformation of the form

\[
\frac{y^\lambda - 1}{\lambda} = \frac{y^{0.3} - 1}{0.3}.
\]

We then fit a model with this transformation applied to the response.

```{r}
gala_model_cox = lm((((Species ^ 0.3) - 1) / 0.3) ~ Area + Elevation + Nearest + Scruz +Adjacent, data = gala)
```

```{r}
plot(fitted(gala_model_cox), resid(gala_model_cox), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

The resulting fitted versus residuals plot looks much better.

Lastly, we return to the `initech` data, and the `initech_fit` model we had used earlier. Recall, that this was the untransformed model, that we used a $\log$ transform to fix.

```{r}
boxcox(initech_fit)
```

Using the Box-Cox method, we see that $\lambda = 0$ is both in the interval, and extremely close to the maximum, which suggests a transformation of the form

\[
  \log(y).
\]

So the Box-Cox method justifies our previous choice of a $\log$ transform!

## Predictor Transformation

In addition to transformation of the response variable, we can also consider transformations of predictor variables. Sometimes these transformations can help with violation of model assumptions, and other times they can be used to simply fit a more flexible model.

```{r, echo = FALSE}
# read data frame from the web
autompg = read.table(
  "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data",
  quote = "\"",
  comment.char = "",
  stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin", "name")
# remove missing data, which is stored as "?"
autompg = subset(autompg, autompg$hp != "?")
# remove the plymouth reliant, as it causes some issues
autompg = subset(autompg, autompg$name != "plymouth reliant")
# give the dataset row names, based on the engine, year and name
rownames(autompg) = paste(autompg$cyl, "cylinder", autompg$year, autompg$name)
# remove the variable for name
autompg = subset(autompg, select = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin"))
# change horsepower from character to numeric
autompg$hp = as.numeric(autompg$hp)
# create a dummary variable for foreign vs domestic cars. domestic = 1.
autompg$domestic = as.numeric(autompg$origin == 1)
# remove 3 and 5 cylinder cars (which are very rare.)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]
# the following line would verify the remaining cylinder possibilities are 4, 6, 8
#unique(autompg$cyl)
# change cyl to a factor variable
autompg$cyl = as.factor(autompg$cyl)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot(mpg ~ hp, data = autompg, col = "dodgerblue", pch = 20, cex = 1.5)
mpg_hp = lm(mpg ~ hp, data = autompg)
abline(mpg_hp, col = "darkorange", lwd = 2)
plot(fitted(mpg_hp), resid(mpg_hp), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot(log(mpg) ~ hp, data = autompg, col = "dodgerblue", pch = 20, cex = 1.5)
mpg_hp_log = lm(log(mpg) ~ hp, data = autompg)
abline(mpg_hp_log, col = "darkorange", lwd = 2)
plot(fitted(mpg_hp_log), resid(mpg_hp_log), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot(log(mpg) ~ log(hp), data = autompg, col = "dodgerblue", pch = 20, cex = 1.5)
mpg_hp_loglog = lm(log(mpg) ~ log(hp), data = autompg)
abline(mpg_hp_loglog, col = "darkorange", lwd = 2)
plot(fitted(mpg_hp_loglog), resid(mpg_hp_loglog), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)
```

### Polynomials

- don't need to change units
- TODO: change story, units

It is well known that the sales response to advertising usually follows a curve reflecting the diminishing returns to advertising expenditure.  As a company increases its advertising expenditure, sales increase, but the rate of increase drops continually after a certain point.  If we consider company sales profits as a function of advertising expenditure, we find that the response function can be very well approximated by a second-order (quadratic) model.  For a particular company, the data on monthly sales $y$ and monthly advertising expenditure $x$, both in hundred thousand dollars, can be found in the data below.

```{r}
marketing = read.csv("../data/marketing.csv")
```

```{r}
plot(sales ~ advert, data = marketing, 
     xlab = "Advert Spending (in $100,000)", ylab = "Sales (in $100,000)",
     pch = 20, cex = 2)
```

We want to fit the model,

\[
  y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i
\]

where $\epsilon_i \sim N(0,\sigma^2)$ for $i = 1, 2, \cdots 21.$ Thus, our $X$ matrix is,

\[
  \begin{bmatrix}
  1      & x_1 & x_1^2    \\
  1      & x_2  & x_2^2   \\
  1      & x_3  & x_3^2   \\
  \ldots & \ldots & \ldots \\
  1      & x_{n}  & x_{n}^2   \\
  \end{bmatrix}
\]

We can then proceed to fit the model as we have in the past for multiple linear regression.

\[
\hat{\beta} = \left(  X^\top X  \right)^{-1}X^\top y.
\]

Our estimates will have the usual properties,

\[
E[\hat{\beta}] = \beta,
\]

and variance

\[
Var[\hat{\beta}] = \sigma^2 \left(  X^\top X  \right)^{-1}.
\]

\[
\hat{\beta}_j \sim N\left(\beta_j, \sigma^2 C_{jj}  \right)
\]

The response $y$ is now a **linear** function of "two" variables which now allows $y$ to be a non-linear function of the original single predictor.


```{r}
mark_mod = lm(sales ~ advert, data = marketing)
summary(mark_mod)

mark_mod_poly2 = lm(sales ~ advert + I(advert ^ 2), data = marketing)
summary(mark_mod_poly2)
```

Here we see that with the first order term in the model, the quadratic term is also significant.

```{r}
n = length(marketing$advert)
X = cbind(rep(1, n), marketing$advert, marketing$advert ^ 2)
t(X) %*% X
solve(t(X) %*% X) %*% t(X) %*% marketing$sales
```

Here we verify the parameter estimates were found as we would expect.

We could also add higher order terms. This is easy to do. Our $X$ matrix simply becomes larger again.

\[
  y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \epsilon_i
\]

\[
  \begin{bmatrix}
  1      & x_1 & x_1^2  & x_1^3    \\
  1      & x_2  & x_2^2  & x_2^3   \\
  1      & x_3  & x_3^2  & x_3^3   \\
  \ldots & \ldots & \ldots & \ldots \\
  1      & x_{n}  & x_{n}^2  & x_{n}^3    \\
  \end{bmatrix}
\]

```{r}
mark_mod_poly3 = lm(sales ~ advert + I(advert ^ 2) + I(advert ^ 3), data = marketing)
summary(mark_mod_poly3)
```

Now we see that with the first and second order terms in the model, the third order term is also significant. But does this make sense practically? The following plot should gives hints as to why it doesn't.

```{r}
plot(sales ~ advert, data = marketing, 
     xlab = "Advert Spending (in $100,000)", ylab = "Sales (in $100,000)",
     pch = 20, cex = 2)
abline(mark_mod, lty = 2, col = "green", lwd = 2)
xplot = seq(0, 16, by = 0.01)
lines(xplot, predict(mark_mod_poly2, newdata = data.frame(advert = xplot)),
      col = "blue", lwd = 2)
lines(xplot, predict(mark_mod_poly3, newdata = data.frame(advert = xplot)),
      col = "red", lty = 3, lwd = 3)
```

The previous plot was made using base graphics in `R`. The next plot was made using the package `ggplot2`, a popular plotting method in `R`.

```{r}
library(ggplot2)
ggplot(data = marketing, aes(x = advert, y = sales)) +
  stat_smooth(method = "lm", se = FALSE, color = "green", formula = y ~ x) +
  stat_smooth(method = "lm", se = FALSE, color = "blue", formula = y ~ x + I(x ^ 2)) +
  stat_smooth(method = "lm", se = FALSE, color = "red", formula = y ~ x + I(x ^ 2)+ I(x ^ 3)) +
  geom_point(colour = "black", size = 3)
```

Note we could fit a polynomial of an arbitrary order,

\[
  y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_{p-1}x_i^{p-1} + \epsilon_i
\]

However, we should be careful about over-fitting, since with a polynomial of degree one less than the number of observations, it is sometimes possible to fit a model perfectly.

```{r}
set.seed(1234)
x = seq(0, 10)
y = 3 + x + 4 * x ^ 2 + rnorm(11, 0, 20)
plot(x, y, ylim = c(-300, 400), cex = 2, pch = 20)
fit = lm(y ~ x + I(x ^ 2))
#summary(fit)
fit_perf = lm(y ~ x + I(x ^ 2) + I(x ^ 3) + I(x ^ 4) + I(x ^ 5) + I(x ^ 6)
               + I(x ^ 7) + I(x ^ 8) + I(x ^ 9) + I(x ^ 10))
summary(fit_perf)
xplot = seq(0, 10, by = 0.1)
lines(xplot, predict(fit, newdata = data.frame(x = xplot)),
      col = "dodgerblue", lwd = 2, lty = 1)
lines(xplot, predict(fit_perf, newdata = data.frame(x = xplot)),
      col = "darkorange", lwd = 2, lty = 2)
```

- can't estimate SE, out of DF. 11 betas, 11 points.

- Here the true relationship is quadratic with noise, but the 10th order polynomial is "perfect". Next chpater we will focus on tradeoff between goodness of fit (minimizing errors) and complexity of model.


- mpg example


- suppose you are a car manufacturer. or  advocacy group>

We now wish to develop a model to predict miles per gallon based on highway mph for a particular brand of SUV.  An experiment is designed in which a test car is driven at speeds ranging from 10 miles per hour to 75 miles per hour. We will fit a polynomial model and use it to predict the average mileage obtained when the car is driven at 55 miles per hour.

- fictional luxury sedan

```{r}
econ = read.csv("../data/fuel_econ.csv")
```

- delete this, we SHOUDL write a function.
In this example, we will be frequently looking at the fitted vs residuals plot, so we will write a function to make our life easier.



We will do the same for added a curve to our data points.

```{r}
plot_econ_curve = function(model){
  plot(mpg ~ mph, data = econ, xlab = "Speed (Miles per Hour)", 
       ylab = "Fuel Efficiency (Miles per Gallon)", col = "dodgerblue", 
       pch = 20, cex =2)
  xplot = seq(10, 75, by = 0.1)
  lines(xplot, predict(model, newdata = data.frame(mph = xplot)),
        col = "darkorange", lwd = 2, lty = 1)
}
```

So now we first fit a simple linear regression to this data.

```{r}
fit1 = lm(mpg ~ mph, data = econ)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot_econ_curve(fit1)
plot(fitted(fit1), resid(fit1), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
  abline(h = 0, col = "darkorange", lwd = 2)
```

Pretty clearly we can do better. 

- yes mpg goes up with mph, but only up to a certain point.

We will now add more polynomial terms until we fit a suitable fit.

```{r}
fit2 = lm(mpg ~ mph + I(mph ^ 2), data = econ)
summary(fit2)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot_econ_curve(fit2)
plot(fitted(fit2), resid(fit2), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
  abline(h = 0, col = "darkorange", lwd = 2)
```

While this model clearly fits much better, and the second order term is significant, we still see a pattern in the fitted versus residuals plot which suggests higher order terms will help. Also, we would expect the curve to flatten as mph increases or decreases, not go sharply downward as we see here.

```{r}
fit3 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3), data = econ)
summary(fit3)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot_econ_curve(fit3)
plot(fitted(fit3), resid(fit3), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
  abline(h = 0, col = "darkorange", lwd = 2)
```

Adding the third order term doesn't seem to help. This makes sense, since what we would like is for the curve to flatten at the extremes. For this we will need an even polynomial term.

```{r}
fit4 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3) + I(mph ^ 4), data = econ)
summary(fit4)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot_econ_curve(fit4)
plot(fitted(fit4), resid(fit4), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
  abline(h = 0, col = "darkorange", lwd = 2)
```

Now we are making progress. The fourth order term is significant with the other terms in the model. Also we are starting to see what we expected for low and high speed. However, there still seems to be a bit of a pattern in the residuals, so we will again try more higher order terms. (We will add the fifth and sixth together, since adding the fifth will be similar to adding the third.)

```{r}
fit6 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3) + I(mph ^ 4) + I(mph ^ 5) + I(mph^6), data = econ)
summary(fit6)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot_econ_curve(fit6)
plot(fitted(fit6), resid(fit6), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
  abline(h = 0, col = "darkorange", lwd = 2)
```

Again the sixth order term is significant with the other terms in the model and here we see less pattern in the residuals plot. Let's now test for which of the previous two models we prefer. Namely we will test.

\[
H_0: \beta_5 = \beta_6 = 0.
\]

```{r}
anova(fit4, fit6)
```

So, this test does not reject the null hypothesis at a level of significance of $\alpha = 0.05$, however the p-value is still rather small, and the fitted versus residuals plot is much better for the model with the sixth order term. This makes the sixth order model a good choice. We could repeat this process one more time.

```{r}
fit8 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3) + I(mph ^ 4) + I(mph ^ 5)
          + I(mph ^ 6) + I(mph ^ 7) + I(mph ^ 8), data = econ)
summary(fit8)
```

```{r, fig.height = 4, fig.width = 8}
par(mfrow = c(1, 2))
plot_econ_curve(fit8)
plot(fitted(fit8), resid(fit8), xlab = "Fitted", ylab = "Residuals", 
     col = "dodgerblue", pch = 20, cex =2)
  abline(h = 0, col = "darkorange", lwd = 2)
```

```{r}
anova(fit6, fit8)
```

Here we would clearly stick with `fit6`. The eighth order term is not significant with the other terms in the model and the F-test does not reject.

- use of `poly()` is the same, demonstrate for 6th order, lead into next chapter.

## TODO

- here only poly of slr, but also in bigger models to generally make more flexible

- reference from devel folder
- check data links